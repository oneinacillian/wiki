{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#the-content-on-this-website-will-give-you-some-detail-on-how-to-optimize-certain-components-use-containers-for-performancescalability-setup-monitoring-and-perform-load-testing","title":"The content on this website will give you some detail on how to optimize certain components, use containers for performance/scalability, setup monitoring and perform load testing","text":""},{"location":"HyperionSnapshotremoteNFS/","title":"The following will be covered here","text":"<ul> <li>Configure NFS server on new (elasticsearch) Hyperion server</li> <li>Configure NFS client on current (elasticsearch) Hyperion server</li> <li>Test NFS share</li> <li>Configure snapshot repository with throttling enabled on current Hyperion server</li> <li>Configure snapshot repository on new Hyperion server  </li> </ul> <p>Please note that you will need to have a VPN configured between the 2 Hyperion server  In this example:  172.168.40.185 = new Hyperion server   172.168.40.160 = current Hyperion server </p>"},{"location":"HyperionSnapshotremoteNFS/#configure-nfs-server-on-new-hyperion-server","title":"Configure NFS server on new Hyperion server","text":"<pre><code>apt-get install nfs-kernel-server\nsudo systemctl start nfs-kernel-server\nsudo systemctl enable nfs-kernel-server\n</code></pre> <p>Continue to set up your export (client access i.e current Hyperion server)</p> <ul> <li>edit /etc/export </li> <li>add the following line /data/snapshots 172.168.40.160(rw,sync,no_root_squash)    /data/snapshots = the path on disk to where NFS should map    172.168.40.160(rw,sync,no_root_squash) = granted access to private host on the VPN (in this case, current hyperion server) </li> </ul>"},{"location":"HyperionSnapshotremoteNFS/#configure-nfs-client-on-current-hyperion-server","title":"Configure NFS client on current Hyperion server","text":"<pre><code>apt-get install nfs-common\nmount -t nfs 172.168.40.185:/data/snapshots /data/snapshots\n</code></pre> <p>Take note for the mount command above:  - 172.168.40.185:/data/snapshots = VPN address of NFS server (new Hyperion) and NFS directory mapping  - /data/snapshots will be the mount point the current Hyperion server will use to access the NFS share on the new Hyperion server </p>"},{"location":"HyperionSnapshotremoteNFS/#test-nfs-share","title":"Test NFS share","text":"<p>On your current Hyperion server, copy a large file to the /data/snapshots directory and confirm it is placed on the new Hyperion server, also within the /data/snapshots directory  If this works, NFS is configure and you can continue to configure your snapshot repository </p>"},{"location":"HyperionSnapshotremoteNFS/#configure-snapshot-repository-with-throttling-enabled-on-current-hyperion-server","title":"Configure snapshot repository with throttling enabled on current Hyperion server","text":"<p>Please note to add your path.repo to your elasticsearch.yml </p> <pre><code>path.repo: [\"/data/snapshots/\"]\n</code></pre> <p>Create a new snapshot repository </p> <pre><code>PUT /_snapshot/hyperion_testnet_snapshots\n{\n  \"type\": \"fs\",\n  \"settings\": {\n    \"location\": \"/data/snapshots\",\n    \"compress\": true,\n    \"max_restore_bytes_per_sec\": \"4gb\",\n    \"max_snapshot_bytes_per_sec\": \"10mb\"    \n  }\n}\n</code></pre> <p>Please note the above </p> <ul> <li>\"type\": \"fs\" = snapshot will be created on a shared filesystem</li> <li>\"location\": \"/data/snapshots\" = the directory we mapped to the NFS share</li> <li>\"max_snapshot_bytes_per_sec\": \"10mb\" = You might need to throttle your snapshot throughput to ensure your network capacity does not impact your current API hosting</li> </ul> <p>You can then go ahead and configure the snapshot policy and kick off the index snapshotting</p> <pre><code>PUT /_slm/policy/hyperion_testnet_snapshot\n{\n  \"schedule\": \"0 0 0 1 * ?\", \n  \"name\": \"hyperion_testnet_snapshot\", \n  \"repository\": \"hyperion_testnet_snapshots\", \n  \"config\": { \n    \"indices\": [\"wax-*\"], \n    \"ignore_unavailable\": false,\n    \"include_global_state\": false\n  }\n}\n</code></pre> <p>note that you can also do a few indexes at one time, depending on how large the disk is on the new Hyperion server for accommodating the snapshot as well as the restored index</p>"},{"location":"HyperionSnapshotremoteNFS/#configure-snapshot-repository-on-new-hyperion-server","title":"Configure snapshot repository on new Hyperion server","text":"<p>Create the snapshot repository on the new hyperion server as well</p> <pre><code>PUT /_snapshot/hyperion_testnet_snapshots\n{\n  \"type\": \"fs\",\n  \"settings\": {\n    \"location\": \"/data/snapshots\",\n    \"compress\": true,\n    \"max_restore_bytes_per_sec\": \"4gb\",\n    \"max_snapshot_bytes_per_sec\": \"10mb\"    \n  }\n}\n</code></pre> <p>You can then restore the snapshots which has been created, either using dev tools, or Kibana UI  Keep in mind that before restoring the snapshot, ensure the elasticsearch user ownership on the /data/snapshots directory </p>"},{"location":"atomic-api/","title":"Atomic API","text":"<p>Take note of ubuntu performance performance when configuring a Atomic instance  Take note of postgresql documentation for database specific operations</p>"},{"location":"atomic-api/#the-following-items-will-be-covered-here","title":"The following items will be covered here","text":"<ul> <li>All own identified issues experienced running Atomic API</li> </ul>"},{"location":"atomic-api/#troubleshooting-rollback-issues","title":"Troubleshooting rollback issues","text":"<p>These are issues we have encountered due to various forks causing rollbacks in Atomic API</p> <p>We have experienced an incident where the rollback kept on failing and retrying, causing the Atomic Indexer to run out of sync</p>"},{"location":"atomic-api/#example_1","title":"Example_1","text":"<p>Typical error in the .pm2/logs/eosio-contract-api-filler-out-0.log would be:</p> <pre><code>2023-07-21T07:31:39.226Z [PID:4145502] [info] : Executed rollback query 29 / 274 \n2023-07-21T07:31:39.226Z [PID:4145502] [warn] : Fork rollback taking longer than expected. Executing query... {\"operation\":\"delete\",\"table\":\"contract_traces\",\"values\":null,\"condition\":{\"str\":\"\\\"global_sequence\\\" = $1 AND \\\"account\\\" = $2\",\"values\":[\"82233095234\",\"atomicassets\"]}}\n2023-07-21T07:40:45.070Z [PID:406251] [info] : Executed rollback query 1 / 274 \n2023-07-21T07:40:45.070Z [PID:406251] [warn] : Fork rollback taking longer than expected. Executing query... {\"operation\":\"delete\",\"table\":\"contract_traces\",\"values\":null,\"condition\":{\"str\":\"\\\"global_sequence\\\" = $1 AND \\\"account\\\" = $2\",\"values\":[\"82233095412\",\"atomicassets\"]}}\n2023-07-21T07:45:50.141Z [PID:406251] [info] : Executed rollback query 2 / 274\n</code></pre> <p>The database table public.contract_traces were queried and noticed I could not retrieve information for account 'atomicassets' when ordered by 'global_sequence' and limited the results to 100 records  It would just timeout and indicated a performance issue  I immediately thought this could could contribute to the constant rollback failures </p> <p><pre><code>SELECT * FROM public.contract_traces \nWHERE \"account\" = 'atomicassets'\nORDER BY \"global_sequence\" DESC\nLIMIT 100;\n</code></pre> I then created an index named idx_contract_traces_account_global_sequence on table public.contract_traces, for fields account and global_sequence, with the global_sequence fields being indexed in a descending order. <pre><code>CREATE INDEX idx_contract_traces_account_global_sequence\nON public.contract_traces (\"account\", \"global_sequence\" DESC);\n</code></pre> The index took approximately 45 minutes to create and the API filler immedaitely resumed blocks processing</p>"},{"location":"containers/","title":"Containers","text":"<p>More a more users attempt to run micro-services for their applications due to the scalability, less overhead and portability. It is also provides a great efficiency</p>"},{"location":"containers/#the-following-will-be-covered-here","title":"The following will be covered here","text":"<ul> <li>Install Docker</li> <li>Configure and use a docker name volume</li> <li>Building and running a state history container</li> <li>Building and running an atomic container service</li> <li>Building and running a hyperion container service</li> </ul>"},{"location":"containers/#install-docker-ce-community-edition","title":"Install docker-ce (community edition)","text":"<ul> <li>uninstall all previous docker utilities</li> <li>Add Docker\u2019s official GPG key</li> <li>Setup up the repository</li> <li>Install latest version of docker</li> <li>Optional (select version of docker to be deployed)</li> </ul>"},{"location":"containers/#uninstall-all-previous-docker-utilities","title":"Uninstall all previous docker utilities","text":"<pre><code>sudo apt-get remove docker docker-engine docker.io containerd runc\n</code></pre>"},{"location":"containers/#add-dockers-official-gpg-key","title":"Add Docker\u2019s official GPG key","text":"<pre><code>sudo mkdir -p /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\n</code></pre>"},{"location":"containers/#set-up-the-repository","title":"Set up the repository","text":"<pre><code>echo \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n</code></pre>"},{"location":"containers/#install-latest-version-of-docker","title":"Install latest version of docker","text":"<pre><code>sudo apt-get update\nsudo apt-get install docker-ce docker-ce-cli containerd.io docker-compose-plugin\n</code></pre>"},{"location":"containers/#optional-select-version-of-docker-to-be-deployed","title":"Optional (select version of docker to be deployed)","text":"<pre><code>apt-cache madison docker-ce\nsudo apt-get install docker-ce=&lt;VERSION_STRING&gt; docker-ce-cli=&lt;VERSION_STRING&gt; containerd.io docker-compose-plugin\n</code></pre>"},{"location":"containers/#configure-and-use-a-docker-name-volume","title":"Configure and use a docker name volume","text":"<p>For your data driven application which require optimized disk configuration (i.e zfs which is raided and optimized), it is best to configure a named docker volume and have it mounted inside your micro-service opposed to bind a volume for use. With docker volumes, the storage is not coupled to the lifecycle of the container, but instead exists outside of it. You\u2019ll also find that volumes don\u2019t increase the size of the Docker container using them. It also provides more flexible backups and data are easier to migrate.</p>"},{"location":"containers/#create-a-docker-volume","title":"Create a docker volume","text":"<ul> <li>Issue command with Docker CLI to created a volume</li> <li>Link up the volume to an external mount point</li> <li>Start a micro-service in daemon mode with access to the external volume</li> </ul>"},{"location":"containers/#issue-command-with-docker-cli-to-created-a-volume","title":"Issue command with Docker CLI to created a volume","text":"<p>NOTE: In this example, I created a docker volume named testvolume to use <pre><code>docker volume create testvolume\n</code></pre></p>"},{"location":"containers/#link-up-the-volume-to-an-external-mount-point","title":"Link up the volume to an external mount point","text":"<p>NOTE: By default all volume will be located in /var/lib/docker/volumes if you have not previously configured your graph driver to point elsewhere. </p> <p>NOTE: In this example your optimized disk will be mounted in linux for example in /apps/datavolume <pre><code>sudo rm -rf /var/lib/docker/volumes/testvolume/_data/\nmkdir /apps/datavolume/testvolume\nsudo ln -s /apps/datavolume/testvolume /var/lib/docker/volumes/testvolume/_data\n</code></pre></p>"},{"location":"containers/#start-a-micro-service-in-daemon-mode-with-access-to-the-external-volume","title":"Start a micro-service in daemon mode with access to the external volume","text":"<p>In this example: </p> <ul> <li>The container name will be: testvolumecontainerservice</li> <li>The container image used will be ubuntu:22.04</li> <li>The named docker volume being used will be testvolume</li> <li>The names docker volume will be accessible by the container through /data <pre><code>docker run -d --name testvolumecontainerservice --mount source=testvolume,target=/data --tty ubuntu:22.04\n</code></pre></li> </ul>"},{"location":"containers/#building-and-running-a-state-history-container","title":"Building and running a state history container","text":""},{"location":"containers/#running-state-history-in-containers","title":"Running State History in containers","text":"<ul> <li>Proceed to a directory from which you would like to build your SHIP images</li> <li>Create the genesis file (testnet's one for example) <pre><code>{\n \"initial_timestamp\": \"2019-12-06T06:06:06.000\",\n \"initial_key\": \"EOS7PmWAXLBaqCzSgbq8cyr2HFztQpwBpXk3djBJA8fyoyUnYM37q\",\n \"initial_configuration\": {\n \"max_block_net_usage\": 1048576,\n \"target_block_net_usage_pct\": 1000,\n \"max_transaction_net_usage\": 524288,\n \"base_per_transaction_net_usage\": 12,\n \"net_usage_leeway\": 500,\n \"context_free_discount_net_usage_num\": 20,\n \"context_free_discount_net_usage_den\": 100,\n \"max_block_cpu_usage\": 200000,\n \"target_block_cpu_usage_pct\": 2500,\n \"max_transaction_cpu_usage\": 150000,\n \"min_transaction_cpu_usage\": 100,\n \"max_transaction_lifetime\": 3600,\n \"deferred_trx_expiration_window\": 600,\n \"max_transaction_delay\": 3888000,\n \"max_inline_action_size\": 4096,\n \"max_inline_action_depth\": 6,\n \"max_authority_depth\": 6\n }\n}\n</code></pre></li> </ul>"},{"location":"containers/#create-a-startup-script-startsh","title":"Create a startup script: start.sh","text":"<pre><code>#!/bin/bash\n\nfor x in /sys/devices/system/cpu/cpu*/cpufreq/;do\necho performance &gt; $x/scaling_governor\ndone\n\nDATADIR=\"/apps/waxdata\"\nNODEOSBINDIR=\"/apps/eosio/2.0/bin\"\n$DATADIR/stop.sh\necho -e \"Starting Nodeos \\n\";\n\n\n$NODEOSBINDIR/nodeos --disable-replay-opts --data-dir $DATADIR --config-dir $DATADIR --genesis-json=/apps/waxdata/genesis.json \"$@\" &gt; $DATADIR/stdout.txt 2&gt; $DATADIR/stderr.txt &amp;  echo $! &gt; $DATADIR/nodeos.pid\n</code></pre>"},{"location":"containers/#create-a-shutdown-script-stopsh","title":"Create a shutdown script: stop.sh","text":"<pre><code>#!/bin/bash\nDIR=\"/apps/waxdata\"\n if [ -f $DIR\"/nodeos.pid\" ]; then\n        pid=`cat $DIR\"/nodeos.pid\"`\n        echo $pid\n        kill $pid\n        echo -ne \"Stoping Nodeos\"\n        while true; do\n            [ ! -d \"/proc/$pid/fd\" ] &amp;&amp; break\n            echo -ne \".\"\n            sleep 1\n        done\n        rm -r $DIR\"/nodeos.pid\"\n        DATE=$(date -d \"now\" +'%Y_%m_%d-%H_%M')\n        if [ ! -d $DIR/logs ]; then\n            mkdir $DIR/logs\n        fi\n        tar -pcvzf $DIR/logs/stderr-$DATE.txt.tar.gz stderr.txt stdout.txt\n        echo -ne \"\\rNodeos Stopped.  \\n\"\n    fi\n</code></pre>"},{"location":"containers/#to-make-use-of-the-named-docker-volume-testvolume-as-explained-previously-create-a-configini-file-in-your-build-directory-and-configure-your-blocks-and-state-dir-on-the-mounted-volume-within-the-container","title":"To make use of the Named Docker volume (testvolume as explained previously), create a config.ini file in your build directory and configure your blocks and state dir on the mounted volume within the container","text":"<pre><code>blocks-dir = /data/blocks-2\nstate-history-dir = /data/state-history\nchain-threads = 8\nwasm-runtime = eos-vm-jit\neos-vm-oc-compile-threads = 4\n...\n...\n...\n</code></pre>"},{"location":"containers/#create-your-dockerfile","title":"Create your Dockerfile","text":"<p><pre><code>FROM ubuntu:18.04\n#COPY 01norecommend /etc/apt/apt.conf.d\nRUN apt-get update &amp;&amp; apt -y install git &amp;&amp; git clone https://github.com/worldwide-asset-exchange/wax-blockchain.git &amp;&amp; cd wax-blockchain \\\n&amp;&amp; git checkout v2.0.12wax02 &amp;&amp; git submodule update --init --recursive &amp;&amp; cd scripts &amp;&amp; yes | ./eosio_build.sh -P &amp;&amp; ./eosio_install.sh &amp;&amp; apt -y install vim \\\n&amp;&amp; mkdir -p /apps/waxdata &amp;&amp; mv /wax-* /apps &amp;&amp; mv /root/eosio* /apps\nCOPY config.ini genesis.json start.sh stop.sh /apps/waxdata/\nRUN chmod +x /apps/waxdata/start.sh &amp;&amp; chmod +x /apps/waxdata/stop.sh\n</code></pre> Execute your build command <pre><code>docker built -t yourrepo:yourtag -f ./yourdockerfile .\n</code></pre></p>"},{"location":"containers/#to-start-your-container-and-expose-ws-and-http-port-example","title":"To start your container and expose WS and HTTP port (example)","text":"<ul> <li>name of container = wax2012wax02</li> <li>expose state history in config.ini on port 8888</li> <li>expose http port in config.ini on port 9876</li> <li>named docker volume named testvolume (follow instructions in previous steps)</li> <li>named volume mounted in container @ /data</li> <li>WAX nodeos image built and tagged as 2.0.12wax02:latest <pre><code>docker run -d --name wax2012wax02 --publish 8888:8888 --publish 9876:9876 --mount source=testvolume,target=/data --tty 2.0.12wax02:latest\n</code></pre> You can now start the nodeos process in the container <pre><code>cd /apps/waxdata\n./start.sh\n</code></pre></li> </ul>"},{"location":"containers/#building-and-running-an-atomic-container-service","title":"Building and running an atomic container service","text":"<p>Create your Dockerfile</p> <pre><code>FROM ubuntu:20.04\nWORKDIR /apps\nARG DEBIAN_FRONTEND=noninteractive\nRUN apt-get update &amp;&amp; apt-get -y install curl &amp;&amp; curl -o file.sh https://deb.nodesource.com/setup_16.x &amp;&amp; chmod +x file.sh &amp;&amp; ./file.sh &amp;&amp; apt-get install -y nodejs \\\n&amp;&amp; npm install pm2 -g &amp;&amp; apt-get -y install wget ca-certificates &amp;&amp; apt-get -y install wget &amp;&amp; wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | apt-key add - \\\n&amp;&amp; sh -c 'echo \"deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main\" &gt; /etc/apt/sources.list.d/pgdg.list' &amp;&amp; apt-get update &amp;&amp; apt-get -y install postgresql postgresql-contrib \\\n&amp;&amp; apt-get -y install vim &amp;&amp; curl -fsSL https://packages.redis.io/gpg | gpg --dearmor -o /usr/share/keyrings/redis-archive-keyring.gpg &amp;&amp; echo \"deb [signed-by=/usr/share/keyrings/redis-archive-keyring.gpg] https://packages.redis.io/deb $(lsb_release -cs) main\" | tee /etc/apt/sources.list.d/redis.list &amp;&amp; apt-get update &amp;&amp; apt-get -y install redis \\\n&amp;&amp; npm install --global yarn &amp;&amp; apt-get -y install git &amp;&amp; git clone https://github.com/pinknetworkx/eosio-contract-api.git &amp;&amp; cd /apps/eosio-contract-api &amp;&amp; yarn install\n</code></pre> <p>Build your docker images</p> <pre><code>docker build -t atomicimage:atomicimage -f ./Dockerfile .\n</code></pre> <p>Map an external volume for your container persistent data</p> <pre><code>docker volume create atomictest\nrm -rf /var/lib/docker/volumes/atomictest/_data/\nmkdir /data/containers/atomictest\nsudo ln -s /data/containers/atomictest /var/lib/docker/volumes/atomictest/_data\n</code></pre> <p>Start up atomic container, publishing the ports necessary to expose the API to the community</p> <pre><code>docker run -d --name atomictest --publish 9000:9000 --publish 9001:9001 --mount source=atomictest,target=/data --tty atomictest:atomictest\n</code></pre> <p>Exec into the container and start postgresql. Get your current data directory</p> <pre><code>postgres@52c2bc761f47:~$ psql\npsql (14.4 (Ubuntu 14.4-1.pgdg20.04+1))\nType \"help\" for help.\n\npostgres=# SHOW data_directory;\n       data_directory        \n-----------------------------\n /var/lib/postgresql/14/main\n(1 row)\n</code></pre> <p>In the above instance, you can see that the data directory is mapped inside the container in /var/lib Ideally you want your data to persistent on a faster volume, which is not bound to the container</p> <p>Move the data directory and update postgresql config</p> <pre><code>service postgresql stop\ncp -var /var/lib/postgresql/ /data/\nnano /etc/postgresql/14/main/postgresql.conf\n****\nupdate data directory\ndata_directory = '/data/postgresql/14/main'             # use data in another directory\n****\n</code></pre> <p>Confirm that your postgresl directory is on the newly specified volume</p> <pre><code>root@36bbb3a2788d:/etc/postgresql/14/main# service postgresql start\n * Starting PostgreSQL 14 database server                                                                                                                                                                                                                                                                                                 [ OK ] \nroot@36bbb3a2788d:/etc/postgresql/14/main# su - postgres\npostgres@36bbb3a2788d:~$ psql\npsql (14.4 (Ubuntu 14.4-1.pgdg20.04+1))\nType \"help\" for help.\n\npostgres=# SHOW data_directory;\n      data_directory      \n--------------------------\n /data/postgresql/14/main\n(1 row)\n</code></pre> <p>Restore n atomic database</p> <p>create the database which you would like to restore and attempt the restore</p> <pre><code>postgres=# CREATE DATABASE \"api-wax-mainnet-atomic-1\";\npg_restore --verbose -Fc -d api-wax-mainnet-atomic-1 -1 /data/backups/atomictest.dump\n</code></pre> <p>NOTE: The backup was create by a guild member that used a specific role as the owner of the database</p> <p>If you encounter issues with the restore process as such: <pre><code>pg_restore: error: could not execute query: ERROR:  role \"wecan_user\" does not exist\n</code></pre></p> <p>Create a role with the same name the backup was created</p> <pre><code>postgres=# CREATE ROLE wecan_user;\n</code></pre> <p>Reattempt the restore and it should complete, unless you encounter issue with the backup</p> <p>View the database once restored by using psql</p> <pre><code>postgres=# \\l+\n                                                                       List of databases\n           Name           |  Owner   | Encoding | Collate |  Ctype  |   Access privileges   |  Size   | Tablespace |                Description                 \n--------------------------+----------+----------+---------+---------+-----------------------+---------+------------+--------------------------------------------\n api-wax-mainnet-atomic-1 | postgres | UTF8     | C.UTF-8 | C.UTF-8 |                       | 20 GB   | pg_default | \n postgres                 | postgres | UTF8     | C.UTF-8 | C.UTF-8 |                       | 8545 kB | pg_default | default administrative connection database\n template0                | postgres | UTF8     | C.UTF-8 | C.UTF-8 | =c/postgres          +| 8377 kB | pg_default | unmodifiable empty database\n                          |          |          |         |         | postgres=CTc/postgres |         |            | \n template1                | postgres | UTF8     | C.UTF-8 | C.UTF-8 | =c/postgres          +| 8529 kB | pg_default | default template for new databases\n                          |          |          |         |         | postgres=CTc/postgres |         |            | \n(4 rows)\n</code></pre> <p>Configure your eosio contract configuration files accordingly. </p> <ul> <li>readers.config.json</li> <li>server.config.json</li> <li>connections.config.json (confirm that your postgresql authentication is configured properly as well as your ship settings)</li> </ul> <p>Start your atomic filler until is has caught up to the main block, then your API as well (run from within your /apps/eosio-contract-api folder).</p> <pre><code>pm2 start ecosystems.config.json --only eosio-contract-api-filler\npm2 start ecosystems.config.json --only eosio-contract-api-server\n</code></pre>"},{"location":"containers/#building-and-running-an-hyperion-container-service","title":"Building and running an hyperion container service","text":""},{"location":"containers/#create-your-dockerfile_1","title":"Create your Dockerfile","text":"<pre><code>FROM ubuntu:20.04\nWORKDIR /apps\nARG DEBIAN_FRONTEND=noninteractive\nRUN apt-get update \\\n&amp;&amp; apt-get -y upgrade \\\n&amp;&amp; apt -y install npm git wget curl vim htop systemctl aptitude git lxc-utils netfilter-persistent sysstat ntp gpg \\\n&amp;&amp; wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | apt-key add - \\\n&amp;&amp; apt-get install apt-transport-https \\\n&amp;&amp; echo \"deb https://artifacts.elastic.co/packages/7.x/apt stable main\" | tee /etc/apt/sources.list.d/elastic-7.x.list \\\n&amp;&amp; apt-get update \\\n&amp;&amp; apt-get install elasticsearch \\\n&amp;&amp; git clone https://github.com/eosrio/hyperion-history-api.git --branch v3.3.5 \\\n&amp;&amp; echo \"-Xms31g\" &gt;&gt; /etc/elasticsearch/jvm.options \\\n&amp;&amp; echo \"-Xmx31g\" &gt;&gt; /etc/elasticsearch/jvm.options \\\n#&amp;&amp; sed -i 's/var\\/lib\\/elasticsearch/data\\/es-data/g' /etc/elasticsearch/elasticsearch.yml \\\n#&amp;&amp; sed -i 's/var\\/log\\/elasticsearch/data\\/es-logs/g' /etc/elasticsearch/elasticsearch.yml \\\n&amp;&amp; service elasticsearch start \\\n&amp;&amp; echo \"xpack.security.enabled: true\" &gt;&gt; /etc/elasticsearch/elasticsearch.yml \\\n&amp;&amp; service elasticsearch restart &amp;&amp; sleep 20 \\\n&amp;&amp; yes | /usr/share/elasticsearch/bin/elasticsearch-setup-passwords auto &gt; credentials.file \\\n&amp;&amp; wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | apt-key add - \\\n&amp;&amp; apt-get update &amp;&amp; apt-get install kibana \\\n&amp;&amp; echo \"server.host: 0.0.0.0\" &gt;&gt; /etc/kibana/kibana.yml \\\n&amp;&amp; cred=$(cat credentials.file | awk '/PASSWORD kibana_system = /{print}' | cut -d '=' -f 2 | sed 's/ //') \\\n&amp;&amp; echo 'elasticsearch.username: \"kibana_system\"' &gt;&gt; /etc/kibana/kibana.yml \\\n&amp;&amp; echo \"elasticsearch.password: \\\"$cred\\\"\" &gt;&gt; /etc/kibana/kibana.yml &gt;&gt; /etc/kibana/kibana.yml \\\n&amp;&amp; service kibana start \\\n&amp;&amp; curl -o nodefile.sh https://deb.nodesource.com/setup_16.x &amp;&amp; chmod +x nodefile.sh \\\n&amp;&amp; apt-get install -y nodejs &amp;&amp; node -v \\\n&amp;&amp; curl -fsSL https://packages.redis.io/gpg | gpg --dearmor -o /usr/share/keyrings/redis-archive-keyring.gpg  \\\n&amp;&amp; echo \"deb [signed-by=/usr/share/keyrings/redis-archive-keyring.gpg] https://packages.redis.io/deb focal main\" | tee /etc/apt/sources.list.d/redis.list \\\n&amp;&amp; apt-get update &amp;&amp; apt-get -y install redis \\\n&amp;&amp; sed -i 's/supervised auto/supervised systemd/g' /etc/redis/redis.conf \\\n&amp;&amp; service redis-server start \\\n&amp;&amp; npm install pm2@latest -g \\\n&amp;&amp; pm2 startup \\\n&amp;&amp; apt-get install curl gnupg apt-transport-https -y \\\n&amp;&amp; curl -1sLf \"https://keys.openpgp.org/vks/v1/by-fingerprint/0A9AF2115F4687BD29803A206B73A36E6026DFCA\" | gpg --dearmor | tee /usr/share/keyrings/com.rabbitmq.team.gpg &gt; /dev/null \\\n&amp;&amp; curl -1sLf \"https://keyserver.ubuntu.com/pks/lookup?op=get&amp;search=0xf77f1eda57ebb1cc\" | gpg --dearmor | tee /usr/share/keyrings/net.launchpad.ppa.rabbitmq.erlang.gpg &gt; /dev/null \\\n&amp;&amp; curl -1sLf \"https://packagecloud.io/rabbitmq/rabbitmq-server/gpgkey\" | gpg --dearmor | tee /usr/share/keyrings/io.packagecloud.rabbitmq.gpg &gt; /dev/null \\\n&amp;&amp; echo \"deb [signed-by=/usr/share/keyrings/net.launchpad.ppa.rabbitmq.erlang.gpg] http://ppa.launchpad.net/rabbitmq/rabbitmq-erlang/ubuntu bionic main\" &gt;&gt; /etc/apt/sources.list.d/rabbitmq.list \\\n&amp;&amp; echo \"deb-src [signed-by=/usr/share/keyrings/net.launchpad.ppa.rabbitmq.erlang.gpg] http://ppa.launchpad.net/rabbitmq/rabbitmq-erlang/ubuntu bionic main\" &gt;&gt; /etc/apt/sources.list.d/rabbitmq.list \\\n&amp;&amp; echo \"deb [signed-by=/usr/share/keyrings/io.packagecloud.rabbitmq.gpg] https://packagecloud.io/rabbitmq/rabbitmq-server/ubuntu/ bionic main\" &gt;&gt; /etc/apt/sources.list.d/rabbitmq.list \\\n&amp;&amp; echo \"deb-src [signed-by=/usr/share/keyrings/io.packagecloud.rabbitmq.gpg] https://packagecloud.io/rabbitmq/rabbitmq-server/ubuntu/ bionic main\" &gt;&gt; /etc/apt/sources.list.d/rabbitmq.list \\\n&amp;&amp; apt-get update -y \\\n&amp;&amp; apt-get install -y erlang-base erlang-asn1 erlang-crypto erlang-eldap erlang-ftp erlang-inets erlang-mnesia erlang-os-mon erlang-parsetools erlang-public-key erlang-runtime-tools erlang-snmp erlang-ssl erlang-syntax-tools erlang-tftp erlang-tools erlang-xmerl \\\n&amp;&amp; apt-get install rabbitmq-server -y --fix-missing \\\n&amp;&amp; service rabbitmq-server start \\\n&amp;&amp; rabbitmq-plugins enable rabbitmq_management \\\n&amp;&amp; rabbitmqctl add_vhost hyperion \\\n&amp;&amp; rabbitmqctl add_user hyper 123456 \\\n&amp;&amp; rabbitmqctl set_user_tags hyper administrator \\\n&amp;&amp; rabbitmqctl set_permissions -p hyperion hyper \".*\" \".*\" \".*\" \\\n&amp;&amp; rabbitmqctl add_vhost /hyperion \\\n&amp;&amp; rabbitmqctl set_permissions -p /hyperion hyper \".*\" \".*\" \".*\" \\\n&amp;&amp; cd /apps/hyperion-history-api &amp;&amp; npm install \\\n&amp;&amp; echo \"service elasticsearch start\" &gt;&gt; /apps/startup.sh \\\n&amp;&amp; echo \"service kibana start\" &gt;&gt; /apps/startup.sh \\\n&amp;&amp; echo \"service rabbitmq-server start\" &gt;&gt; /apps/startup.sh \\\n&amp;&amp; echo \"service redis-server start\" &gt;&gt; /apps/startup.sh \\\n&amp;&amp; echo \"service elasticsearch stop\" &gt;&gt; /apps/stop.sh \\\n&amp;&amp; echo \"service kibana stop\" &gt;&gt; /apps/stop.sh \\\n&amp;&amp; echo \"service rabbitmq-server stop\" &gt;&gt; /apps/stop.sh \\\n&amp;&amp; echo \"service redis-server stop\" &gt;&gt; /apps/stop.sh \\\n&amp;&amp; chmod +x /apps/startup.sh \\\n&amp;&amp; chmod +x /apps/stop.sh\nRUN apt-get install -y systemd\n</code></pre> <p>build your docker image</p> <pre><code>docker build -t hyperiontest:latest -f ./Dockerfile .\n</code></pre> <p>Map an external volume for your container persistent data</p> <pre><code>docker volume create hyperiontest\nrm -rf /var/lib/docker/volumes/hyperiontest/_data/\nmkdir /data/containers/hyperiontest\nsudo ln -s /data/containers/hyperiontest /var/lib/docker/volumes/hyperiontest/_data\n</code></pre> <p>Start up atomic container, publishing the ports necessary to expose the API to the community</p> <pre><code>docker run -d --name hyperiontest --publish 7000:7000 --publish 5601:5601 --mount source=hyperiontest,target=/data --tty hyperiontest:latest\n</code></pre> <p>Execute to a terminal in the hyperion container</p> <pre><code>docker exec -ti hyperiontest /bin/bash\n</code></pre> <p>Move the data and log volumes for elasticsearch to your named volume</p> <pre><code>chown -R elasticsearch:elasticsearch /data\nmkdir -p /data/es-data\nmkdir -p /data/es-logs\ncp -var /var/lib/elasticsearch/* /data/es-data\ncp -var /var/log/elasticsearch/* /data/es-logs\n</code></pre> <p>Modify the configuration (elasticsearch) to reference the new volumes for data and log management (elasticsearch.yml)</p> <pre><code>path.data: /data/es-data\npath.logs: /data/es-logs\n</code></pre> <p>Startup elasticsearch and all dependant services. You will have a stop and startup.sh file located in /apps</p> <pre><code>bash /apps/startup.sh\n</code></pre> <p>Once all the application has been started, proceed to kibana through your web console on the port you expose (in this example, ::5601)</p> <p>NOTE: Should you have trouble starting Kibana, reset the elastic passwords (yes | /usr/share/elasticsearch/bin/elasticsearch-setup-passwords auto &gt; credentials.file, take not of the password and update your password reference in /etc/kibana/kibana.yml)</p> <p>Restore snapshot (if not indexing from the 1st block). In this example, I exposed a snapshot via http \"http://23.88.71.224:8080/downloads/\" and named the repository \"gipi-repo\". Perform this using DEV tools under management</p> <p>Create repository <pre><code>PUT _snapshot/gipi-repo\n{\n   \"type\": \"url\",\n   \"settings\": {\n       \"url\": \"http://23.88.71.224:8080/downloads/\",\n       \"max_restore_bytes_per_sec\": \"1gb\",\n       \"max_snapshot_bytes_per_sec\": \"1gb\"\n   }\n}\n</code></pre></p> <p>List all snapshots <pre><code>GET _snapshot/gipi-repo/_all\n</code></pre></p> <p>Set the repository url as trusted by elasticsearch (modification in elasticsearch.yml) and restart elasticsearch <pre><code>repositories.url.allowed_urls: \"http://23.88.71.224:8080/downloads/\"\n</code></pre></p> <p>Start a restore of a snapshot (DEV tools) <pre><code>POST _snapshot/gipi-repo/daily_snapshot-qu4_uyc0tuotoaj1x3lsjq/_restore\n{\n  \"indices\": \"*,-.*\"\n}\n</code></pre></p> <p>Wait for the restore to complete, then configure your hyperion contract information</p> <ul> <li>/apps/hyperion-history-api/connections.json NOTE RabbitMQ password will be what you have defined in the Dockerfile build  <pre><code>{\n  \"amqp\": {\n    \"host\": \"127.0.0.1:5672\",\n    \"api\": \"127.0.0.1:15672\",\n    \"protocol\": \"http\",\n    \"user\": \"&lt;from dockerfile&gt;\",\n    \"pass\": \"&lt;from dockerfile&gt;\",\n    \"vhost\": \"hyperion\",\n    \"frameMax\": \"0x10000\"\n  },\n  \"elasticsearch\": {\n    \"protocol\": \"http\",\n    \"host\": \"127.0.0.1:9200\",\n    \"ingest_nodes\": [\n      \"127.0.0.1:9200\"\n    ],\n    \"user\": \"elastic\",\n    \"pass\": \"&lt;from credentials file in /apps&gt;\"\n  },\n  \"redis\": {\n    \"host\": \"127.0.0.1\",\n    \"port\": \"6379\"\n  },\n  \"chains\": {\n    \"wax\": {\n      \"name\": \"Wax\",\n      \"ship\": \"ws://172.168.40.201:10876\",\n      \"http\": \"http://172.168.40.201:9888\",\n      \"chain_id\": \"f16b1833c747c43682f4386fca9cbb327929334a762755ebec17f6f23c9b8a12\",\n      \"WS_ROUTER_HOST\": \"127.0.0.1\",\n      \"WS_ROUTER_PORT\": 7001\n    }\n  }\n}\n</code></pre></li> <li>/apps/hyperion-history-api/chains/wax.config.json <pre><code>{\n  \"api\": {\n    \"enabled\": true,\n    \"pm2_scaling\": 1,\n    \"node_max_old_space_size\": 1024,\n    \"chain_name\": \"wax\",\n    \"server_addr\": \"172.168.40.100\",\n    \"server_port\": 7000,\n    \"server_name\": \"hyperion-test.oiac.io\",\n    \"provider_name\": \"Oneinacillian\",\n    \"provider_url\": \"https://oiac.io\",\n    \"chain_api\": \"\",\n    \"push_api\": \"\",\n    \"chain_logo_url\": \"\",\n    \"enable_caching\": true,\n    \"cache_life\": 1,\n    \"limits\": {\n      \"get_actions\": 1000,\n      \"get_voters\": 100,\n      \"get_links\": 1000,\n      \"get_deltas\": 1000,\n      \"get_trx_actions\": 200\n    },\n    \"access_log\": false,\n    \"chain_api_error_log\": false,\n    \"custom_core_token\": \"\",\n    \"enable_export_action\": false,\n    \"disable_rate_limit\": false,\n    \"rate_limit_rpm\": 1000,\n    \"rate_limit_allow\": [],\n    \"disable_tx_cache\": false,\n    \"tx_cache_expiration_sec\": 3600,\n    \"v1_chain_cache\": [\n      {\n        \"path\": \"get_block\",\n        \"ttl\": 3000\n      },\n      {\n        \"path\": \"get_info\",\n        \"ttl\": 500\n      }\n    ]\n  },\n  \"indexer\": {\n    \"enabled\": true,\n    \"node_max_old_space_size\": 4096,\n    \"start_on\": 0,\n    \"stop_on\": 0,\n    \"rewrite\": false,\n    \"purge_queues\": false,\n    \"live_reader\": true,\n    \"live_only_mode\": false,\n    \"abi_scan_mode\": false,\n    \"fetch_block\": true,\n    \"fetch_traces\": true,\n    \"disable_reading\": false,\n    \"disable_indexing\": false,\n    \"process_deltas\": true,\n    \"disable_delta_rm\": true\n  },\n  \"settings\": {\n    \"preview\": false,\n    \"chain\": \"wax\",\n    \"eosio_alias\": \"eosio\",\n    \"parser\": \"1.8\",\n    \"auto_stop\": 0,\n    \"index_version\": \"v1\",\n    \"debug\": false,\n    \"bp_logs\": false,\n    \"bp_monitoring\": false,\n    \"ipc_debug_rate\": 60000,\n    \"allow_custom_abi\": false,\n    \"rate_monitoring\": true,\n    \"max_ws_payload_mb\": 1024,\n    \"ds_profiling\": false,\n    \"auto_mode_switch\": false,\n    \"hot_warm_policy\": false,\n    \"custom_policy\": \"\",\n    \"bypass_index_map\": true,\n    \"index_partition_size\": 10000000,\n    \"es_replicas\": 0\n  },\n  \"blacklists\": {\n    \"actions\": [],\n    \"deltas\": []\n  },\n  \"whitelists\": {\n    \"actions\": [],\n    \"deltas\": [],\n    \"max_depth\": 10,\n    \"root_only\": false\n  },\n  \"scaling\": {\n    \"readers\": 1,\n    \"ds_queues\": 1,\n    \"ds_threads\": 1,\n    \"ds_pool_size\": 1,\n    \"indexing_queues\": 1,\n    \"ad_idx_queues\": 1,\n    \"dyn_idx_queues\": 1,\n    \"max_autoscale\": 4,\n    \"batch_size\": 5000,\n    \"resume_trigger\": 5000,\n    \"auto_scale_trigger\": 20000,\n    \"block_queue_limit\": 10000,\n    \"max_queue_limit\": 100000,\n    \"routing_mode\": \"round_robin\",\n    \"polling_interval\": 10000\n  },\n  \"features\": {\n    \"streaming\": {\n      \"enable\": true,\n      \"traces\": true,\n      \"deltas\": true\n    },\n    \"tables\": {\n      \"proposals\": true,\n      \"accounts\": true,\n      \"voters\": true\n    },\n    \"index_deltas\": true,\n    \"index_transfer_memo\": true,\n    \"index_all_deltas\": true,\n    \"deferred_trx\": false,\n    \"failed_trx\": false,\n    \"resource_limits\": false,\n    \"resource_usage\": false\n  },\n  \"prefetch\": {\n    \"read\": 50,\n    \"block\": 100,\n    \"index\": 500\n  },\n  \"plugins\": {}\n}\n</code></pre></li> </ul> <p>Start your api indexer to allow your data to index from the SHIP node</p> <pre><code>./run.sh wax-indexer\n</code></pre> <p>Once it has indexed all the outstanding blocks, you can expose the api</p> <pre><code>./run.sh wax-api\n</code></pre>"},{"location":"curator/","title":"Curator","text":""},{"location":"curator/#elasticsearch-curator","title":"Elasticsearch Curator","text":"<pre><code>root@191b27da5fae:/apps# pip install elasticsearch-curator\nCollecting elasticsearch-curator\n  Downloading elasticsearch_curator-5.8.4-py2.py3-none-any.whl (106 kB)\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 106 kB 9.9 MB/s \nCollecting elasticsearch&lt;8.0.0,&gt;=7.12.0\n  Downloading elasticsearch-7.17.6-py2.py3-none-any.whl (385 kB)\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 385 kB 70.5 MB/s \nCollecting requests&gt;=2.25.1\n  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62 kB 13.8 MB/s \nCollecting click&lt;8.0,&gt;=7.0\n  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 82 kB 11.1 MB/s \nCollecting urllib3==1.26.4\n  Downloading urllib3-1.26.4-py2.py3-none-any.whl (153 kB)\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 153 kB 149.5 MB/s \nCollecting certifi&gt;=2020.12.5\n  Downloading certifi-2022.9.14-py3-none-any.whl (162 kB)\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 162 kB 147.8 MB/s \nCollecting requests-aws4auth&gt;=1.0.1\n  Downloading requests_aws4auth-1.1.2-py2.py3-none-any.whl (24 kB)\nCollecting boto3&gt;=1.17.57\n  Downloading boto3-1.24.75-py3-none-any.whl (132 kB)\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 132 kB 137.8 MB/s \nCollecting six&gt;=1.15.0\n  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\nCollecting pyyaml==5.4.1\n  Downloading PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB)\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 662 kB 129.7 MB/s \nCollecting voluptuous&gt;=0.12.1\n  Downloading voluptuous-0.13.1-py3-none-any.whl (29 kB)\nCollecting idna&lt;4,&gt;=2.5\n  Downloading idna-3.4-py3-none-any.whl (61 kB)\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61 kB 1.2 MB/s \nCollecting charset-normalizer&lt;3,&gt;=2\n  Downloading charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\nCollecting jmespath&lt;2.0.0,&gt;=0.7.1\n  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\nCollecting s3transfer&lt;0.7.0,&gt;=0.6.0\n  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 79 kB 58.7 MB/s \nCollecting botocore&lt;1.28.0,&gt;=1.27.75\n  Downloading botocore-1.27.75-py3-none-any.whl (9.1 MB)\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9.1 MB 149.6 MB/s \nCollecting python-dateutil&lt;3.0.0,&gt;=2.1\n  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 247 kB 143.9 MB/s \nInstalling collected packages: certifi, urllib3, elasticsearch, idna, charset-normalizer, requests, click, six, requests-aws4auth, jmespath, python-dateutil, botocore, s3transfer, boto3, pyyaml, voluptuous, elasticsearch-curator\nSuccessfully installed boto3-1.24.75 botocore-1.27.75 certifi-2022.9.14 charset-normalizer-2.1.1 click-7.1.2 elasticsearch-7.17.6 elasticsearch-curator-5.8.4 idna-3.4 jmespath-1.0.1 python-dateutil-2.8.2 pyyaml-5.4.1 requests-2.28.1 requests-aws4auth-1.1.2 s3transfer-0.6.0 six-1.16.0 urllib3-1.26.4 voluptuous-0.13.1\n</code></pre> <p>Prepare your curator config and actions file</p> <pre><code>touch /root/.curator/curator.yml\ntouch /root/.curator/actions.yml\n</code></pre> <p>Configure your curator for elastic interaction</p> <pre><code>client:\n  hosts:\n    - 127.0.0.1\n  port: 9200\n  url_prefix:\n  use_ssl: false\n  certificate:\n  client_cert:\n  client_key:\n  ssl_no_validate: False\n  username: \"\"\n  password: \"\"\n  timeout: 30\n  master_only: False\nlogging:\n  loglevel: INFO\n  logfile:\n  logformat: default\n  blacklist: ['elasticsearch', 'urllib3']\n</code></pre> <p>Configure your actions file to determine actions against indexes</p> <pre><code>actions:\n  1:\n    action: delete_indices\n    description: &gt;-\n      Delete indices older than 4 hours (based on index name)\n    options:\n      ignore_empty_list: True\n      disable_action: False\n    filters:\n    - filtertype: pattern\n      kind: regex\n      value: '^(wax-action-|wax-delta-).*$'\n    - filtertype: age\n      source: creation_date\n      direction: older\n      timestring: '%Y.%m.%d.%h'\n      unit: hours\n      unit_count: 5\n</code></pre> <p>Perform a dry run to test which indexes will receive an action</p> <pre><code>curator /root/.curator/actions.yml --dry-run\n</code></pre> <p>-------------------------------------------- Test Setup --------------------------------------------  Recently did a full hyperion load from SHIP in the past 2 days to simulator the use of elastic curator  NOTE: Any actions performed against the index will be revelant on the age of the index. Which means  if you restore all indexes using a snapshot or copy of metadata, age will be recent  Test was performed using a container, on elastic 7.17.6  I am deleting indexes older than 2 days =&gt;  -------------------------------------------- Test Setup -------------------------------------------- </p> <pre><code>actions:\n  1:\n    action: delete_indices\n    description: &gt;-\n      Delete indices older than 4 hours (based on index name)\n    options:\n      ignore_empty_list: True\n      disable_action: False\n    filters:\n    - filtertype: pattern\n      kind: regex\n      value: '^(wax-action-|wax-delta-).*$'\n    - filtertype: age\n      source: creation_date\n      direction: older\n      timestring: '%Y.%m.%d.%h'\n      unit: days\n      unit_count: 2 \n</code></pre> <p>The result of this would be (in my case as I started the hyperion load approximately 2 days ago)</p> <pre><code>2022-09-18 17:45:46,210 INFO      DRY-RUN MODE.  No changes will be made.\n2022-09-18 17:45:46,210 INFO      (CLOSED) indices may be shown that may not be acted on by action \"delete_indices\".\n2022-09-18 17:45:46,210 INFO      DRY-RUN: delete_indices: wax-action-v1-000001 with arguments: {}\n2022-09-18 17:45:46,210 INFO      DRY-RUN: delete_indices: wax-action-v1-000002 with arguments: {}\n2022-09-18 17:45:46,210 INFO      DRY-RUN: delete_indices: wax-action-v1-000003 with arguments: {}\n2022-09-18 17:45:46,210 INFO      DRY-RUN: delete_indices: wax-delta-v1-000001 with arguments: {}\n2022-09-18 17:45:46,210 INFO      DRY-RUN: delete_indices: wax-delta-v1-000002 with arguments: {}\n2022-09-18 17:45:46,210 INFO      DRY-RUN: delete_indices: wax-delta-v1-000003 with arguments: {}\n2022-09-18 17:45:46,211 INFO      Action ID: 1, \"delete_indices\" completed.\n2022-09-18 17:45:46,211 INFO      Job completed.\n</code></pre>"},{"location":"haproxy/","title":"HAProxy","text":"<p>To expose your Hyperion, History or Atomic API you need a load balancer. There are several community balancers available for use and the following ones will be explained</p> <ul> <li>HAProxy</li> <li>NGINX - not yet complete</li> <li>Traefik - not yet complete</li> </ul> <p>Other components that will be covered</p> <ul> <li>ACL some examples will be added soon</li> <li>Rate limiting with explanation and examples (stick-tables, tarpit, etc)</li> <li>Cors setup</li> <li>Letsencrypt to generate a public certificate</li> </ul>"},{"location":"haproxy/#haproxy","title":"HAProxy","text":""},{"location":"haproxy/#install","title":"Install","text":"<p>Installation on Ubuntu (this installation was performed on ubuntu 18.04, but will be the same on 20.04) <pre><code>1. sudo apt install --no-install-recommends software-properties-common\n2. sudo add-apt-repository ppa:vbernat/haproxy-2.4 -y\n</code></pre> Result =&gt; <pre><code>Reading package lists... Done\nBuilding dependency tree\nReading state information... Done\nsoftware-properties-common is already the newest version (0.96.24.32.18).\nThe following packages were automatically installed and are no longer required:\n  liblua5.3-0 linux-image-4.15.0-156-generic linux-modules-4.15.0-156-generic linux-modules-extra-4.15.0-156-generic\nUse 'sudo apt autoremove' to remove them.\n0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\nroot@vultr:~# sudo add-apt-repository ppa:vbernat/haproxy-2.4 -y\nHit:1 https://apprepo.vultr.com/debian buster InRelease\nGet:2 http://ppa.launchpad.net/vbernat/haproxy-2.4/ubuntu bionic InRelease [20.8 kB]\nHit:3 http://de.clouds.archive.ubuntu.com/ubuntu bionic InRelease\nHit:4 http://security.ubuntu.com/ubuntu bionic-security InRelease\nGet:5 http://ppa.launchpad.net/vbernat/haproxy-2.4/ubuntu bionic/main i386 Packages [976 B]\nGet:6 http://ppa.launchpad.net/vbernat/haproxy-2.4/ubuntu bionic/main amd64 Packages [972 B]\nGet:7 http://ppa.launchpad.net/vbernat/haproxy-2.4/ubuntu bionic/main Translation-en [704 B]\nHit:8 http://de.clouds.archive.ubuntu.com/ubuntu bionic-updates InRelease\nHit:9 http://de.clouds.archive.ubuntu.com/ubuntu bionic-backports InRelease\nFetched 23.4 kB in 1s (40.1 kB/s)\nReading package lists... Done\n</code></pre> <pre><code>3. sudo apt install haproxy=2.4.\\*\n</code></pre> Result =&gt; <pre><code>Reading package lists... Done\nBuilding dependency tree\nReading state information... Done\nSelected version '2.4.17-1ppa1~bionic' (HAProxy 2.4:18.04/bionic [amd64]) for 'haproxy'\nThe following packages were automatically installed and are no longer required:\n  linux-image-4.15.0-156-generic linux-modules-4.15.0-156-generic linux-modules-extra-4.15.0-156-generic\nUse 'sudo apt autoremove' to remove them.\nThe following additional packages will be installed:\n  libpcre2-8-0\nSuggested packages:\n  vim-haproxy haproxy-doc\nThe following NEW packages will be installed:\n  haproxy libpcre2-8-0\n0 upgraded, 2 newly installed, 0 to remove and 0 not upgraded.\nNeed to get 1,763 kB of archives.\nAfter this operation, 4,209 kB of additional disk space will be used.\nDo you want to continue? [Y/n] y\nGet:1 http://ppa.launchpad.net/vbernat/haproxy-2.4/ubuntu bionic/main amd64 haproxy amd64 2.4.17-1ppa1~bionic [1,584 kB]\nGet:2 http://de.clouds.archive.ubuntu.com/ubuntu bionic/universe amd64 libpcre2-8-0 amd64 10.31-2 [179 kB]\nFetched 1,763 kB in 1s (3,242 kB/s)\nSelecting previously unselected package libpcre2-8-0:amd64.\n(Reading database ... 146169 files and directories currently installed.)\nPreparing to unpack .../libpcre2-8-0_10.31-2_amd64.deb ...\nUnpacking libpcre2-8-0:amd64 (10.31-2) ...\nSelecting previously unselected package haproxy.\nPreparing to unpack .../haproxy_2.4.17-1ppa1~bionic_amd64.deb ...\nUnpacking haproxy (2.4.17-1ppa1~bionic) ...\nSetting up libpcre2-8-0:amd64 (10.31-2) ...\nSetting up haproxy (2.4.17-1ppa1~bionic) ...\nInstalling new version of config file /etc/haproxy/errors/500.http ...\nInstalling new version of config file /etc/haproxy/haproxy.cfg ...\nInstalling new version of config file /etc/logrotate.d/haproxy ...\nInstalling new version of config file /etc/rsyslog.d/49-haproxy.conf ...\nProcessing triggers for libc-bin (2.27-3ubuntu1.6) ...\nProcessing triggers for systemd (237-3ubuntu10.53) ...\nProcessing triggers for man-db (2.8.3-2ubuntu0.1) ...\nProcessing triggers for rsyslog (8.32.0-1ubuntu4.2) ...\nProcessing triggers for ureadahead (0.100.0-21) ...\n</code></pre></p> <p>You should now have the latest version of 2.4.x installed</p> <p>Ensure that your haproxy installation is working and is running <pre><code>4. service haproxy status\n</code></pre> Result =&gt; <pre><code>* haproxy.service - HAProxy Load Balancer\n   Loaded: loaded (/lib/systemd/system/haproxy.service; enabled; vendor preset: enabled)\n   Active: active (running) since Thu 2022-07-14 19:03:36 UTC; 3min 43s ago\n     Docs: man:haproxy(1)\n           file:/usr/share/doc/haproxy/configuration.txt.gz\n Main PID: 721 (haproxy)\n    Tasks: 3 (limit: 2314)\n   CGroup: /system.slice/haproxy.service\n           |-721 /usr/sbin/haproxy -sf 725 -x /run/haproxy/admin.sock -Ws -f /etc/haproxy/haproxy.cfg -p /run/haproxy.pid -S /run/haproxy-master.sock\n           `-775 /usr/sbin/haproxy -sf 725 -x /run/haproxy/admin.sock -Ws -f /etc/haproxy/haproxy.cfg -p /run/haproxy.pid -S /run/haproxy-master.sock\n</code></pre> There are four essential sections to an HAProxy configuration file. They are global, defaults, frontend, and backend. These four sections define how the server as a whole performs, what your default settings are, and how client requests are received and routed to your backend servers.</p> <p>The structure is as follows <pre><code>global\n    # global settings here\n\ndefaults\n    # defaults here\n\nfrontend\n    # a frontend that accepts requests from clients\n\nbackend\n    # servers that fulfill the requests\n</code></pre> The following section will explain a typical configuration for Wax node use</p> <p>NOTE: That the only difference here from the default configuration for the global section provided by the template is the enablement of CORS. This will be explained in more detail later</p> <p>Global and Defaults:  <pre><code>global\n        log /dev/log    local0\n        log /dev/log    local1 notice\n        chroot /var/lib/haproxy\n        stats socket /run/haproxy/admin.sock mode 660 level admin expose-fd listeners\n        stats timeout 30s\n        user haproxy\n        group haproxy\n        lua-load /etc/haproxy/cors.lua\n        daemon\n\n        # Default SSL material locations\n        ca-base /etc/ssl/certs\n        crt-base /etc/ssl/private\n\n        # See: https://ssl-config.mozilla.org/#server=haproxy&amp;server-version=2.0.3&amp;config=intermediate\n        ssl-default-bind-ciphers ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384\n        #ssl-default-bind-ciphersuites TLS_AES_128_GCM_SHA256:TLS_AES_256_GCM_SHA384:TLS_CHACHA20_POLY1305_SHA256\n        ssl-default-bind-options ssl-min-ver TLSv1.2 no-tls-tickets\n\ndefaults\n        log     global\n        mode    http\n        option  httplog\n        option  dontlognull\n        timeout connect 5000\n        timeout client  50000\n        timeout server  50000\n        errorfile 400 /etc/haproxy/errors/400.http\n        errorfile 403 /etc/haproxy/errors/403.http\n        errorfile 408 /etc/haproxy/errors/408.http\n        errorfile 500 /etc/haproxy/errors/500.http\n        errorfile 502 /etc/haproxy/errors/502.http\n        errorfile 503 /etc/haproxy/errors/503.http\n        errorfile 504 /etc/haproxy/errors/504.http\n</code></pre></p> <p>NOTE: This is a basic example you need to allow both http/s traffic to be accepted to be offloaded to the backend/s</p> <p>Frontend: <pre><code>frontend eoshttps\n        bind *:443 ssl crt /etc/haproxy/certs alpn h2,http/1.1\n        http-request set-header X-Forwarded-Proto https\n        acl &lt;identifier_for_rule&gt; hdr(host) -i &lt;hostheader_to_offload&gt;\n        http-request lua.cors\n        http-response lua.cors \"GET,PUT,POST\" \"*\"\n        use_backend &lt;backend_system&gt; if &lt;identifier_for_rule&gt;\n\nfrontend eoshttp\n        bind *:80\n        mode http\n        acl &lt;identifier_for_rule&gt; hdr(host) -i &lt;hostheader_to_offload&gt;\n        http-request lua.cors\n        http-response lua.cors \"GET,PUT,POST\" \"*\"\n        use_backend &lt;backend_system&gt; if &lt;identifier_for_rule&gt;\n</code></pre> Explanation for the values above:</p> <ul> <li>identifier_for_rule can be any rule name you provide</li> <li>backend_system can be any name of the backend system you will configure next so that the front-end requests know where to offload data</li> <li>hostheader_to_offload the hostheader received by the call made to the proxy</li> <li>For tls communication, a certificate needs to be loaded so that the traffic between the (public client) and the proxy can be encrypted. In the configuration above, the certificate needs to be placed in /etc/haproxy/certs. WAX requires TLSv1.2 as a minimum</li> </ul> <p>NOTE: In this example I am offloading to 2 hyperion instances, both with their API's exposed on port 7000</p> <p>Backend: <pre><code>backend &lt;backend_system&gt;\n        server wax 172.168.40.100:7000 maxconn 3000\n        server wax 172.168.40.101:7000 maxconn 3000\n</code></pre></p>"},{"location":"haproxy/#rate-limiting","title":"Rate limiting","text":"<p>Rate limiting can reduce your traffic and potentially improve throughput by reducing the number of records sent to a service over a given period of time.  This can also be used to mitigate abuse where your frontend and backends will be overloaded by abusers, affecting the availability of your service  </p>"},{"location":"haproxy/#setting-the-maximum-connections","title":"Setting the Maximum Connections","text":"<p>Use the maxconn parameter on a server line to cap the number of concurrent connections that will be sent. Look at the example above which limits it too 3000 connections</p> <p>If all 3000 connections are being used both servers, or in other words 6000 connections are active, then new connections will have to wait in line for a slot to free up. This means that the servers themselves won\u2019t become overloaded.</p>"},{"location":"haproxy/#sliding-window-rate-limiting","title":"Sliding Window Rate Limiting","text":"<p>Limit the number of requests that a user can make within a certain period of time to a certain url in this example  In your frontend configuration =&gt; <pre><code>http-request track-sc0 src table wax_api_servers if { url_beg /v1/chain/push_transaction }\nhttp-request tarpit deny_status 429 if { sc_http_req_rate(0) gt 20 } { url_beg /v1/chain/push_transaction }\ntimeout tarpit 10s\n</code></pre> Explanation for the components above: </p> <ul> <li>track-sc0 tracking specific event. Multiple rules can exist, which means track-sc1, track-sc2, etc </li> <li>src table wax_api_servers the table which should be referenced for the evaluated data. Please note that for each rule (i.e. track-sc1,track-sc2), it needs to use a unique table </li> <li>if { url_beg /v1/chain/push_transaction } will only apply the rule when the client calls a specific URL. In this case the API being hit resides in /v1/chain/push_transaction </li> <li>tarpit will stall the request for a period of time before returning an error response </li> <li>deny_status 429 if { sc_http_req_rate(0) gt 20 } { url_beg /v1/chain/push_transaction } return status code 429 if the request rate in table track-sc0 is higher than the allowed value specified in the stick-table</li> </ul> <p>In your backend configuration =&gt; <pre><code>backend wax_api_servers\n        stick-table type ip size 1m expire 60s store http_req_rate(20s)\n</code></pre> Explanation for the components above: </p> <ul> <li>wax_api_servers will be the table backend configured in the frontend reference </li> <li>stick-table type ip directive creates a key-value store for storing counters like the HTTP request rate per client. The key is the client\u2019s IP address, as configured by the type parameter, which is used to store and aggregate that client\u2019s number of requests. he counters begin to be recorded as soon as the IP is added </li> <li>size 1m we are allowing 1 million records to be stored in the table </li> <li>expire 60s amount of time before the table record expires and is removed after a period of inactivity by the client </li> <li>http_req_rate(20s) counts the requests over a period of 20 seconds. Meaning, in the frontend example above if { sc_http_req_rate(0) gt 20 }, if the request is greater than 20 within a timewindow of 20 seconds as specified in the backend http_req_rate(20s), it will deny the request with a 429 deny_status 429 which will wait 10s timeout tarpit 10s before sending the response back</li> </ul>"},{"location":"haproxy/#letsencrypt-to-generate-a-public-certificate","title":"Letsencrypt to generate a public certificate","text":"<p>Install certbot</p> <pre><code>sudo add-apt-repository -y ppa:certbot/certbot\nsudo apt-get install -y certbot\n</code></pre> <p>Add the following in your proxy configuration (I used haproxy in this example)</p> <pre><code>frontend fe-scalinglaravel\n       bind *:80\n       acl letsencrypt-acl path_beg /.well-known/acme-challenge/\n       use_backend letsencrypt-backend if letsencrypt-acl\n\nbackend letsencrypt-backend\n       server letsencrypt 127.0.0.1:8889\n</code></pre> <p>Open up the firewall to accept public callback on letsencrypt backend</p> <pre><code>firewall-cmd --add-port=80/tcp --permanent\nservice firewalld reload\n</code></pre> <p>Ensure that your DNS record exist. In this example, I created wikibuild.oiac.io to point to the public interface</p> <pre><code>root@vultr:/etc/haproxy# dig wikibuild.oiac.io\n\n; &lt;&lt;&gt;&gt; DiG 9.16.1-Ubuntu &lt;&lt;&gt;&gt; wikibuild.oiac.io\n;; global options: +cmd\n;; Got answer:\n;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 39166\n;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1\n\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags:; udp: 65494\n;; QUESTION SECTION:\n;wikibuild.oiac.io.             IN      A\n\n;; ANSWER SECTION:\nwikibuild.oiac.io.      1505    IN      A       45.77.55.245\n\n;; Query time: 0 msec\n;; SERVER: 127.0.0.53#53(127.0.0.53)\n;; WHEN: Sat Jul 23 18:21:35 UTC 2022\n;; MSG SIZE  rcvd: 62\n</code></pre> <p>Ensure that your certbot has started</p> <pre><code>service certbot start\n</code></pre> <p>Generate a certificate using certbot configure your e-mail address accordingly</p> <pre><code>certbot certonly --standalone -d wikibuild.oiac.io --non-interactive --agree-tos --email ********@gmail.com --http-01-port=8889\n</code></pre> <p>Result=&gt;</p> <pre><code>root@vultr:/etc/haproxy# certbot certonly --standalone -d wikibuild.oiac.io --non-interactive --agree-tos --email ********@gmail.com --http-01-port=8889\nSaving debug log to /var/log/letsencrypt/letsencrypt.log\nPlugins selected: Authenticator standalone, Installer None\nObtaining a new certificate\nPerforming the following challenges:\nhttp-01 challenge for wikibuild.oiac.io\nWaiting for verification...\nCleaning up challenges\n\nIMPORTANT NOTES:\n - Congratulations! Your certificate and chain have been saved at:\n   /etc/letsencrypt/live/wikibuild.oiac.io/fullchain.pem\n   Your key file has been saved at:\n   /etc/letsencrypt/live/wikibuild.oiac.io/privkey.pem\n   Your cert will expire on 2022-10-21. To obtain a new or tweaked\n   version of this certificate in the future, simply run certbot\n   again. To non-interactively renew *all* of your certificates, run\n   \"certbot renew\"\n - If you like Certbot, please consider supporting our work by:\n\n   Donating to ISRG / Let's Encrypt:   https://letsencrypt.org/donate\n   Donating to EFF:                    https://eff.org/donate-le\n</code></pre> <p>Take note of your full certificate chain and private key which is listed in the result run of the above</p> <p>Combine your certificate chain and private key, then save it to where haproxy will reference it for SSL termination</p> <pre><code>CERTIFICATE=wikibuild.oiac.io\nmkdir -p /etc/haproxy/certs\ncat /etc/letsencrypt/live/$CERTIFICATE/fullchain.pem /etc/letsencrypt/live/$CERTIFICATE/privkey.pem &gt; /etc/haproxy/certs/$CERTIFICATE.pem\n</code></pre> <p>Check your certificate</p> <pre><code>openssl x509 -in wikibuild.oiac.io.pem -text -noout\n</code></pre> <p>Check your key as well which is now contained in the same file </p> <pre><code>openssl rsa -in wikibuild.oiac.io.pem -check\n</code></pre> <p>Your publicly signed certificate is now ready to be used</p>"},{"location":"haproxy/#cors-setup","title":"Cors setup","text":"<p>CORS is a mechanism for whitelisting domains that would otherwise have been restricted by the browser\u2019s same-origin policy.</p> <p>Please find the CORS module here  Copy the file content to a file named cors.lua in /etc/haproxy</p> <p>Add the following to your proxy configuration</p> <pre><code>global\n        lua-load /etc/haproxy/cors.lua\n\nfrontend eoshttps\n        http-request lua.cors\n        http-response lua.cors \"GET,PUT,POST\" \"*\"\n\nfrontend eoshttp\n        http-request lua.cors\n        http-response lua.cors \"GET,PUT,POST\" \"*\"\n</code></pre>"},{"location":"hyperion/","title":"Hyperion","text":"<p>Take note of ubuntu performance performance when configuring a Atomic instance</p>"},{"location":"hyperion/#the-following-will-be-covered-here","title":"The following will be covered here","text":"<ul> <li>Create an indexing snapshot</li> <li>Restore a snapshot</li> <li>Optimize indexing operations for bulk processing - not yet complete</li> <li>Reset elastic credentials</li> <li>Upgrade Elasticsearch from 7.x to 8.x using upgrade assistant - not yet complete</li> <li>Recover Missing documents via a script</li> <li>Recover Missing documents manually (failures during indexing operations)</li> <li>pin up container on secondary host to participate in indexing operations - not yet complete</li> </ul>"},{"location":"hyperion/#create-an-indexing-snapshot","title":"Create an indexing snapshot","text":"<ol> <li>Add path.repo: [\"/data/snapshots\"] to elasticsearch.yml where you would like to store your snapshots</li> <li>Stop your hyperion indexer and api process</li> <li>Restart elasticsearch</li> <li>Log into kibana</li> <li> <p>Using dev tools under management, create a snapshot repository <pre><code>PUT /_snapshot/my_repository\n{\n  \"type\": \"fs\",\n  \"settings\": {\n    \"location\": \"/data/snapshots\",\n    \"compress\": true,\n    \"max_restore_bytes_per_sec\": \"1gb\",\n    \"max_snapshot_bytes_per_sec\": \"1gb\"    \n  }\n}\n</code></pre> Please note that the above settings has been optimized for throughput   You should now see your repository being visible for snapshots  </p> </li> <li> <p>Make sure that you set ownership to elasticsearch on the snapshot directory <pre><code>chown -R elasticsearch:elasticsearch /data/snapshots\n</code></pre></p> </li> <li> <p>Create a daily snapshot policy <pre><code>PUT /_slm/policy/daily_snapshot\n{\n  \"schedule\": \"0 30 1 * * ?\", \n  \"name\": \"daily_snapshot\", \n  \"repository\": \"my_repository\", \n  \"config\": { \n    \"indices\": [\"wax-action-v1-000001\"], \n    \"ignore_unavailable\": false,\n    \"include_global_state\": false\n  },\n  \"retention\": { \n    \"expire_after\": \"30d\", \n    \"min_count\": 5, \n    \"max_count\": 50 \n  }\n}\n</code></pre> You should now see the configure snapshot policy  </p> </li> <li> <p>Trigger a run to make sure that the index/s you selected, backed up successfully </p> </li> </ol> <p></p>"},{"location":"hyperion/#restore-a-snapshot-from-url","title":"Restore a snapshot from url","text":"<p>Configure your repository URL from which you want to restart in your allowed list within the elasticsearch.yml <pre><code>repositories.url.allowed_urls: \"http://23.88.71.224:8080/downloads/\"\n</code></pre> </p>"},{"location":"hyperion/#optimize-indexing-operations-for-bulk-processing","title":"Optimize indexing operations for bulk processing","text":""},{"location":"hyperion/#reset-elastic-credentials","title":"Reset elastic credentials","text":"<p><pre><code>yes | /usr/share/elasticsearch/bin/elasticsearch-setup-passwords auto &gt; credentials.file\n</code></pre> You can then have a copy of the following stored in credentials.file</p> <ul> <li>apm_system</li> <li>kibana_system (configured in kibana.yml)</li> <li>kibana</li> <li>logstash_system</li> <li>beats_system</li> <li>remote_monitoring_user</li> <li>elastic (used to log into Kibana UI)</li> </ul>"},{"location":"hyperion/#upgrade-elasticsearch-from-7x-to-8x-do-not-perform-the-upgrade-yet-as-this-process-is-being-writtentested","title":"Upgrade Elasticsearch from 7.x to 8.x ==&gt; !!Do not perform the upgrade yet as this process is being written/tested!!","text":"<ol> <li>Create snapshot of all your elastic indexes</li> <li>Migrate system indices (if not already ticked)</li> <li>Review deprecated settings and resole issues for critical errors (if not already ticked)</li> <li>Address API deprecations if necessary</li> </ol>"},{"location":"hyperion/#create-snapshot-of-all-your-elastic-indexes","title":"Create snapshot of all your elastic indexes","text":"<p>Create the following policy</p> <p><pre><code>PUT /_slm/policy/upgrade_snapshot\n{\n  \"schedule\": \"0 30 1 * * ?\", \n  \"name\": \"upgrade_snapshot\", \n  \"repository\": \"my_repository\", \n  \"config\": { \n    \"indices\": [\"wax-*\"], \n    \"ignore_unavailable\": false,\n    \"include_global_state\": false\n  },\n  \"retention\": { \n    \"expire_after\": \"30d\", \n    \"min_count\": 5, \n    \"max_count\": 50 \n  }\n}\n</code></pre> Kick off the above policy to backup all your wax indexes  </p> <p>Indexes (at the time of the upgrade on testnet)</p> <pre><code>wax-abi-v1\nwax-action-v1-000001\nwax-action-v1-000002\nwax-action-v1-000003\nwax-action-v1-000004\nwax-action-v1-000005\nwax-action-v1-000006\nwax-action-v1-000007\nwax-action-v1-000008\nwax-action-v1-000009\nwax-action-v1-000010\nwax-action-v1-000011\nwax-action-v1-000012\nwax-action-v1-000013\nwax-action-v1-000014\nwax-action-v1-000015\nwax-action-v1-000016\nwax-action-v1-000017\nwax-block-v1\nwax-delta-v1-000001\nwax-delta-v1-000002\nwax-delta-v1-000003\nwax-delta-v1-000004\nwax-delta-v1-000005\nwax-delta-v1-000006\nwax-delta-v1-000007\nwax-delta-v1-000008\nwax-delta-v1-000009\nwax-delta-v1-000010\nwax-delta-v1-000011\nwax-delta-v1-000012\nwax-delta-v1-000013\nwax-delta-v1-000014\nwax-delta-v1-000015\nwax-delta-v1-000016\nwax-delta-v1-000017\nwax-link-v1\nwax-logs-v1\nwax-perm-v1\nwax-table-accounts-v1\nwax-table-proposals-v1\nwax-table-voters-v1\n</code></pre> <p>Using the optimized repository settings specific earlier in Create an indexing snapshot, it took 428s to snapshot 315GB of data</p> <p> </p> <p>To Upgrade you elasticsearch, perform the following:</p> <ol> <li>Disable shard allocation <pre><code>PUT _cluster/settings\n{\n  \"persistent\": {\n    \"cluster.routing.allocation.enable\": \"primaries\"\n  }\n}\n</code></pre></li> <li>Stop non-essential indexing and perform a flush <pre><code>POST /_flush\n</code></pre></li> <li>Temporarily stop the tasks associated with active machine learning jobs and datafeeds <pre><code>POST _ml/set_upgrade_mode?enabled=true\n</code></pre></li> </ol> <p>still to check <pre><code>PUT /_cluster/settings\n{\n  \"transient\": {\n    \"cluster.routing.allocation.enable\": null\n  },\n  \"persistent\": {\n    \"cluster.routing.allocation.enable\": null\n  }\n}\n</code></pre></p> <ol> <li>Shutdown node <pre><code>sudo -i service elasticsearch stop\n</code></pre></li> <li> <p>Register the new stable package apt source for elastic-8.x <pre><code>rm -rf /etc/apt/sources.list.d/elastic-7.x.list\necho \"deb https://artifacts.elastic.co/packages/8.x/apt stable main\" | tee /etc/apt/sources.list.d/elastic-8.x.list\napt-get update\n</code></pre></p> <p>You should see https://artifacts.elastic.co/packages/8.x/apt stable InRelease in the repository being listed</p> </li> <li> <p>Upgrade elasticsearch to 8.x <pre><code>root@26b4315c7b5c:/etc/apt/sources.list.d# apt-get upgrade elasticsearch\nReading package lists... Done\nBuilding dependency tree\nReading state information... Done\nCalculating upgrade... Done\nThe following packages will be upgraded:\n  curl dirmngr elasticsearch erlang-asn1 erlang-base erlang-crypto erlang-eldap erlang-ftp erlang-inets erlang-mnesia erlang-os-mon erlang-parsetools erlang-public-key erlang-runtime-tools erlang-snmp\n  erlang-ssl erlang-syntax-tools erlang-tftp erlang-tools erlang-xmerl git git-man gnupg gnupg-l10n gnupg-utils gpg gpg-agent gpg-wks-client gpg-wks-server gpgconf gpgsm gpgv kibana libcurl3-gnutls\n  libcurl4 libhttp-daemon-perl libpython2.7-minimal libpython2.7-stdlib libpython3.8 libpython3.8-minimal libpython3.8-stdlib libssl-dev libssl1.1 linux-libc-dev openssl python2.7 python2.7-minimal\n  python3.8 python3.8-minimal rabbitmq-server redis redis-server redis-tools\n53 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\nNeed to get 870 MB of archives.\nAfter this operation, 606 MB of additional disk space will be used.\n</code></pre></p> </li> </ol>"},{"location":"hyperion/#recover-missing-documents-via-a-script","title":"Recover Missing documents via a script","text":"<p>It often can happen that during the indexing operation you encountered a component failure which causes the indexing operation to miss certain blocks during the indexing.</p>"},{"location":"hyperion/#recover-via-a-script","title":"Recover via a script","text":"<p>One of you valued community members has provided a python based utility to automate the recovery of documents which were lost during the indexing operations.</p> <p>Please follow this link as a first attempt to resolve all missing documents </p> <p>To confirm that no block data is missing, connect to Kibana and open dev tools under management   Run the following POST command to view the block histogram. Note that each bucket contains 10,000,000 documents  This way you can also easily verify which bucket has missing data  <pre><code>POST wax-block-*/_search\n{\n  \"aggs\": {\n    \"block_histogram\": {\n      \"histogram\": {\n        \"field\": \"block_num\",\n        \"interval\": 10000000,\n        \"min_doc_count\": 1\n      },\n      \"aggs\": {\n        \"max_block\": {\n          \"max\": {\n            \"field\": \"block_num\"\n          }\n        }\n      }\n    }\n  },\n  \"size\": 0,\n  \"query\": {\n    \"match_all\": {}\n  }\n}\n</code></pre> Check the block histogram on the results pane to see if any documents are missing.  The expectation here is that each bucket prior to the bucket in which documents are indexed, are fully populated with 10,000,000 documents  Run the wax indexer utility until you catch up with headblock the run the following to query hyperion health <pre><code>http://hyperion.oiac.io/v2/health\n</code></pre> The expected respone will be to have your head_block_num, last_indexed_block and total_indexed_blocks in sync   Should this not be the case, start the manual recovery process as explained in the below section</p>"},{"location":"hyperion/#recover-missing-documents-manually","title":"Recover Missing documents manually","text":"<p>In this section, I will explain the manual process of finding the blocks and recovering the manually via the wax-indexer operations.</p> <p>Determine the amount of documents stored in each bucket, which should contain 10000000 items <pre><code>POST wax-block-*/_search\n{\n  \"aggs\": {\n    \"block_histogram\": {\n      \"histogram\": {\n        \"field\": \"block_num\",\n        \"interval\": 10000000,\n        \"min_doc_count\": 1\n      },\n      \"aggs\": {\n        \"max_block\": {\n          \"max\": {\n            \"field\": \"block_num\"\n          }\n        }\n      }\n    }\n  },\n  \"size\": 0,\n  \"query\": {\n    \"match_all\": {}\n  }\n} \n</code></pre> Result will look like follows =&gt; <pre><code>\"aggregations\" : {\n    \"block_histogram\" : {\n      \"buckets\" : [\n        {\n          \"key\" : 0.0,\n          \"doc_count\" : 9999998,\n          \"max_block\" : {\n            \"value\" : 9999999.0\n          }\n        },\n        {\n          \"key\" : 1.0E7,\n          \"doc_count\" : 10000000,\n          \"max_block\" : {\n            \"value\" : 1.9999999E7\n          }\n        },\n        {\n          \"key\" : 2.0E7,\n          \"doc_count\" : 10000000,\n          \"max_block\" : {\n            \"value\" : 2.9999999E7\n          }\n        },\n        {\n          \"key\" : 3.0E7,\n          \"doc_count\" : 10000000,\n          \"max_block\" : {\n            \"value\" : 3.9999999E7\n          }\n        },\n        {\n          \"key\" : 4.0E7,\n          \"doc_count\" : 10000000,\n          \"max_block\" : {\n            \"value\" : 4.9999999E7\n          }\n        },\n        {\n          \"key\" : 5.0E7,\n          \"doc_count\" : 10000000,\n          \"max_block\" : {\n            \"value\" : 5.9999999E7\n          }\n        },\n        {\n          \"key\" : 6.0E7,\n          \"doc_count\" : 10000000,\n          \"max_block\" : {\n            \"value\" : 6.9999999E7\n          }\n        },\n        {\n          \"key\" : 7.0E7,\n          \"doc_count\" : 10000000,\n          \"max_block\" : {\n            \"value\" : 7.9999999E7\n          }\n        },\n        {\n          \"key\" : 8.0E7,\n          \"doc_count\" : 10000000,\n          \"max_block\" : {\n            \"value\" : 8.9999999E7\n          }\n        },\n        {\n          \"key\" : 9.0E7,\n          \"doc_count\" : 10000000,\n          \"max_block\" : {\n            \"value\" : 9.9999999E7\n          }\n        },\n        {\n          \"key\" : 1.0E8,\n          \"doc_count\" : 10000000,\n          \"max_block\" : {\n            \"value\" : 1.09999999E8\n          }\n        },\n        {\n          \"key\" : 1.1E8,\n          \"doc_count\" : 10000000,\n          \"max_block\" : {\n            \"value\" : 1.19999999E8\n          }\n        },\n        {\n          \"key\" : 1.2E8,\n          \"doc_count\" : 10000000,\n          \"max_block\" : {\n            \"value\" : 1.29999999E8\n          }\n        },\n        {\n          \"key\" : 1.3E8,\n          \"doc_count\" : 10000000,\n          \"max_block\" : {\n            \"value\" : 1.39999999E8\n          }\n        },\n        {\n          \"key\" : 1.4E8,\n          \"doc_count\" : 10000000,\n          \"max_block\" : {\n            \"value\" : 1.49999999E8\n          }\n        },\n        {\n          \"key\" : 1.5E8,\n          \"doc_count\" : 10000000,\n          \"max_block\" : {\n            \"value\" : 1.59999999E8\n          }\n        },\n        {\n          \"key\" : 1.6E8,\n          \"doc_count\" : 3904020,\n          \"max_block\" : {\n            \"value\" : 1.639114E8\n          }\n        }\n      ]\n    }\n  }\n</code></pre> In this example above, you can see that each bucket starts at a key which defines the starting block number  In this case, there are 16 buckets, 15 should contain 10,000,000 blocks each, with the last one still indexing  If at any time you do not have a doc_count of 10,000,000 in each bucket (except for the last one), there are document missing and validators will mark the system as incomplete </p> <p>Example of 2 buckets having missing data</p> <p><pre><code>        {\n          \"key\" : 1.4E8,\n          \"doc_count\" : 9950000,\n          \"max_block\" : {\n            \"value\" : 1.49999999E8\n          }\n        },\n        {\n          \"key\" : 1.5E8,\n          \"doc_count\" : 9998000,\n          \"max_block\" : {\n            \"value\" : 1.59999999E8\n          }\n        },\n</code></pre> Above you can see  - Bucket starting at 140000000 has 50,000 documents missing - Bucket starting at 150000000 has 2,000 documents missing</p> <p>Next would be to view the individual buckets to narrow down the searches to find and index the missing blocks  Let us start by working on the 1st index which has missing data  Narrow down the search to 1,000,000 documents </p> <p>Run the following command to retrieve 1,000,000 items at a time on bucket starting at 140,000,000 <pre><code>POST wax-block-*/_search\n{\n  \"aggs\": {\n    \"block_histogram\": {\n      \"histogram\": {\n        \"field\": \"block_num\",\n        \"interval\": 1000000,\n        \"min_doc_count\": 1\n      },\n      \"aggs\": {\n        \"max_block\": {\n          \"max\": {\n            \"field\": \"block_num\"\n          }\n        }\n      }\n    }\n  },\n  \"size\": 0,\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        {\n          \"range\": {\n            \"block_num\": {\n              \"gte\": 140000000,\n              \"lte\": 150000000\n            }\n          }\n        }\n      ]\n    }\n  }\n}\n</code></pre>  Viewing the results of the above query, either the blocks can be missing in multiple buckets, or in only 1, but the recovery process remains the same  The recovery process that be run for the entire bucket as well, but could be time consuming  For the purpose of this document, I will recover missing blocks from 2 buckets  <pre><code>        {\n          \"key\" : 1.45E8,\n          \"doc_count\" : 975000,\n          \"max_block\" : {\n            \"value\" : 1.45999999E8\n          }\n        },\n        {\n          \"key\" : 1.46E8,\n          \"doc_count\" : 975000,\n          \"max_block\" : {\n            \"value\" : 1.46999999E8\n          }\n        },\n</code></pre> Above, you can see</p> <ul> <li>bucket starting 145000000 (represented by key 1.45E8) has 25,000 blocks missing </li> <li>bucket starting 146000000 (represented by key 1.46E8) has 25,000 blocks missing </li> </ul> <p>To start recovery the missing blocks, we need to use the wax indexer app provided as part of the hyperion install, and which is used to run the main indexing operations  Stop your current indexing <pre><code>./stop.sh wax-indexer\n</code></pre> Then modify the following in your wax.config.json to recover the first bucket missing 25,000 blocks  <pre><code>  \"indexer\": {\n    \"enabled\": true,\n    \"node_max_old_space_size\": 8192,\n    \"start_on\": 145975000,\n    \"stop_on\": 146000000,\n    \"rewrite\": true,\n    \"purge_queues\": true,\n    \"live_reader\": false,\n    \"live_only_mode\": false,\n    \"abi_scan_mode\": false,\n    \"fetch_block\": true,\n    \"fetch_traces\": true,\n    \"disable_reading\": false,\n    \"disable_indexing\": false,\n    \"process_deltas\": true,\n    \"disable_delta_rm\": true\n  },\n</code></pre></p> <ul> <li>disable live_reader</li> <li>enable purge_queues and rewrite</li> <li>configure the start_on and stop_on parameters which will tell the indexer which blocks to recover (note examples used for recovery above)</li> </ul> <p>Start your idexing operations to recover the first 25,000 blocks.  <pre><code>./run.sh wax-indexer\n</code></pre> Once completed, modify the start_on and stop_on parameters to recover missing blocks for the next bucket (note examples used for recovery above)  NOTE: to restart your wax-indexer after each change to take effect</p> <p><pre><code>  \"indexer\": {\n    \"enabled\": true,\n    \"node_max_old_space_size\": 8192,\n    \"start_on\": 145975000,\n    \"stop_on\": 146000000,\n    \"rewrite\": true,\n    \"purge_queues\": true,\n    \"live_reader\": false,\n    \"live_only_mode\": false,\n    \"abi_scan_mode\": false,\n    \"fetch_block\": true,\n    \"fetch_traces\": true,\n    \"disable_reading\": false,\n    \"disable_indexing\": false,\n    \"process_deltas\": true,\n    \"disable_delta_rm\": true\n  },\n</code></pre> Once all the blocks has been restored</p> <ul> <li>disable purge_queues and rewrite</li> <li>enable live_reader</li> </ul> <p>Restart and let it catch up to headblock again.</p>"},{"location":"hyperion/#spin-up-container-on-secondary-host-to-participate-in-indexing-operations","title":"Spin up container on secondary host to participate in indexing operations","text":""},{"location":"installation/","title":"Installations","text":"<p>This document will contain various WAX node installations/configuration instructions.  Please note that some of these steps might differe between the type of infrastructure and operating systems you intend to host for your wax node </p> <ul> <li>WAX api installations at AWS</li> <li>WAX state history node installation at AWS</li> </ul>"},{"location":"installation/#wax-api-installations-at-aws","title":"WAX api installations at AWS","text":"<p>One of your esteemed guild members provided a complete guide on WAX api node installations for AWS </p> <p>Please follow this link for instructions on the installation/configuration process</p>"},{"location":"installation/#wax-state-history-node-installation-at-aws","title":"WAX state history node installation at AWS","text":"<p>One of your esteemed guild members provided complete guide on WAX state history installations for AWS </p> <p>Please follow this link for instructions on the installation/configuration process</p>"},{"location":"locust/","title":"Loadtest setup using Locust","text":"<p>Locust is an open source load testing tool, which is usefull for load testing against your API's to understand throughput and mitigate API abuse</p>"},{"location":"locust/#the-following-will-be-covered-here","title":"The following will be covered here","text":"<ul> <li>Stage the directory for locust</li> <li>Configure a basic test using the Locustfile.py</li> <li>Deploy Locust as a single container</li> <li>Run a load test using the Locust interface</li> <li>Deploy Locust Master and slaves as a microservice  - not yet complete</li> <li>Configure a load test against Atomic API  - not yet complete</li> <li>Configure a load test against Hyperion API  - not yet complete</li> <li>Configure a load test against History API  - not yet complete</li> </ul>"},{"location":"locust/#configure-a-basic-test-using-the-locustfilepy","title":"Configure a basic test using the Locustfile.py","text":"<p>You will stage the python module and import to Locust to define your test. In this example it will perform a single get method against your specific URL+(/v1/chain/get_info). Note that this file needs to be mounted when starting the container service</p> <pre><code>from locust import HttpUser, task, between\n\nclass QuickstartUser(HttpUser):\n    wait_time = between(5, 9)\n    @task\n    def getAPI(self):\n        self.client.get(\"/v1/chain/get_info\")\n        self.client.get(\"/v1/chain/get_table_rows\")\n        self.client.get(\"/v1/chain/get_info\")\n</code></pre>"},{"location":"locust/#deploy-locust-as-a-single-container","title":"Deploy Locust as a single container","text":"<pre><code>docker run -p 8089:8089 -v $PWD:/mnt/locust locustio/locust -f /mnt/locust/locustfile.py\nResponse =&gt;\n[2022-07-12 18:59:48,317] 425d50cb7761/INFO/locust.main: Starting web interface at http://0.0.0.0:8089 (accepting connections from all network interfaces)\n[2022-07-12 18:59:48,326] 425d50cb7761/INFO/locust.main: Starting Locust 2.10.1\n</code></pre> <ul> <li>The application will bind to port 8089 which will automatically be exposed to the operating system</li> <li>If you would like to access the GUI for Locust remotely, be sure to enable it on your server firewall</li> </ul> <pre><code>firewall-cmd --add-port=8089/tcp --permanent\nservice firewalld reload\n</code></pre>"},{"location":"locust/#run-a-load-test-using-the-locust-interface","title":"Run a load test using the Locust interface","text":"<p>You can access Locust by hitting your public/private ip on the exposed port above</p> <p></p> <p>Run Locust</p> <p></p> <p>Change runtime parameters</p> <p></p> <p>View results</p> <p></p>"},{"location":"monitoring/","title":"Monitoring (Prometheus/Grafana)","text":"<p>Prometheus is a free software application used for event monitoring and alerting. It records real-time metrics in a time series database built using a HTTP pull model, with flexible queries and real-time alerting.</p>"},{"location":"monitoring/#the-following-will-be-covered-here","title":"The following will be covered here","text":"<ul> <li>Manual Install prometheus on Ubuntu</li> <li>Add remote endpoint to allow prometheus to scrape proxy (haproxy) stats </li> <li>Install and configure Grafana</li> <li>Import HA proxy dashboard to have visibility on query and history traffic</li> <li>Configure Node exporter for full visibility in on host resources - not yet complete</li> <li>Configure a telegram bot</li> <li>Configure contact point</li> <li>Configure Alerting in Grafana to post alert messages to telegram</li> <li>Automate deploy using Ansible playbook - not yet complete</li> </ul>"},{"location":"monitoring/#manual-install-prometheus-on-ubuntu","title":"Manual Install prometheus on Ubuntu","text":"<pre><code>#------install Prometheus begin------\nexport RELEASE=\"2.2.1\"\nsudo useradd --no-create-home --shell /bin/false prometheus\nsudo mkdir /etc/prometheus\nsudo mkdir /var/lib/prometheus\nsudo chown prometheus:prometheus /etc/prometheus\nsudo chown prometheus:prometheus /var/lib/prometheus\ncd /opt/\nwget https://github.com/prometheus/prometheus/releases/download/v2.26.0/prometheus-2.26.0.linux-amd64.tar.gz\nsha256sum prometheus-2.26.0.linux-amd64.tar.gz\ntar -xvf prometheus-2.26.0.linux-amd64.tar.gz\ncd prometheus-2.26.0.linux-amd64\nsudo cp /opt/prometheus-2.26.0.linux-amd64/prometheus /usr/local/bin/\nsudo cp /opt/prometheus-2.26.0.linux-amd64/promtool /usr/local/bin/\nsudo chown prometheus:prometheus /usr/local/bin/prometheus\nsudo chown prometheus:prometheus /usr/local/bin/promtool\nsudo cp -r /opt/prometheus-2.26.0.linux-amd64/consoles /etc/prometheus\nsudo cp -r /opt/prometheus-2.26.0.linux-amd64/console_libraries /etc/prometheus\nsudo cp -r /opt/prometheus-2.26.0.linux-amd64/prometheus.yml /etc/prometheus\nsudo chown -R prometheus:prometheus /etc/prometheus/consoles\nsudo chown -R prometheus:prometheus /etc/prometheus/console_libraries\nsudo chown -R prometheus:prometheus /etc/prometheus/prometheus.yml\necho '\n[Unit]\nDescription=Prometheus\nWants=network-online.target\nAfter=network-online.target\n\n[Service]\nUser=prometheus\nGroup=prometheus\nType=simple\nExecStart=/usr/local/bin/prometheus \\\n    --config.file /etc/prometheus/prometheus.yml \\\n    --storage.tsdb.path /var/lib/prometheus/ \\\n    --web.console.templates=/etc/prometheus/consoles \\\n    --web.console.libraries=/etc/prometheus/console_libraries\n\n[Install]\nWantedBy=multi-user.target' &gt; /etc/systemd/system/prometheus.service\nsudo systemctl daemon-reload\nsudo systemctl start prometheus\nsudo systemctl enable prometheus\napt-get install -y firewalld\nfirewall-cmd --add-port=9090/tcp --permanent\nservice firewalld reload\n#------------------------------------\n</code></pre>"},{"location":"monitoring/#add-remote-endpoint-to-allow-prometheus-to-scrape-proxy-stats","title":"Add remote endpoint to allow prometheus to scrape proxy stats","text":"<p>Example use here will be the following setup from which you would like to scrape.</p> <p><pre><code>frontend stats\n        mode http\n        bind 172.168.30.10:8404\n        option http-use-htx\n        http-request use-service prometheus-exporter if { path /metrics }\n        stats enable\n        stats uri /stats\n        stats refresh 10s\n</code></pre> What you will need to have is your monitor instance connecting to the instance yo scrape via a private like (check wireguard setup in this documentation) To start scrape the metrics which is exposed via proxy, add the following to your prometheus.yml located in /etc/prometheus/ <pre><code>  - job_name: wax_hyperion_test_ha_proxy\n    static_configs:\n      - targets: ['172.168.30.10:8404']\n</code></pre></p>"},{"location":"monitoring/#install-and-configure-grafana","title":"Install and configure Grafana","text":"<p>Grafana will be configured here to use prometheus as a datasource for viewing the time scaled data (data over time for as long as your retention is set on the datasource db for prometheus)</p> <pre><code>#------install Grafana begin------\nwget -q -O - https://packages.grafana.com/gpg.key | sudo apt-key add -\necho \"deb https://packages.grafana.com/oss/deb stable main\" | sudo tee -a /etc/apt/sources.list.d/grafana.list\nsudo apt-get update\nsudo apt-get install -y grafana\nsudo systemctl start grafana-server\nsudo systemctl enable grafana-server.service\nfirewall-cmd --add-port=3000/tcp --permanent\nservice firewalld reload\nservice grafana-server start\n#---------------------------------\n</code></pre> <p>Connect to the Grafana interface via your public/private ip, or a DNS address you have configured</p> <p></p> <p>Select to configure a prometheus datasource for your monitoring data</p> <p></p> <p>Configure your prometheus datasource</p> <p> </p>"},{"location":"monitoring/#import-ha-proxy-dashboard-to-have-visibility-on-query-and-history-traffic","title":"Import HA proxy dashboard to have visibility on query and history traffic","text":"<p>Load you haproxy2 dashboard</p> <p>Please find the dashboard json to import here</p> <p>  Continue with the prompts until the dashboard is installed</p> <p>The dashboard should be visible here</p> <p>  You can expand any statistic to get a full representation of how your ingress/egress is performing, as well as health statistics of the proxy service</p>"},{"location":"monitoring/#configure-a-telegram-bot","title":"Configure a telegram bot","text":"<p>Open Telegram, and create a new Bot by searching for @BotFather. Select the certified one as per the identification badge </p> <p> </p> <p>Click on start </p> <p> </p> <p>Create new bot </p> <p> </p> <p>Choose a name for the bot </p> <p> </p> <p>Create username for the bot </p> <p> </p> <p>Take note of API token and bot link </p> <p> </p> <p>Start the bot </p> <p> </p> <p>Create a new group in telegram </p> <p> </p> <p>Add the bot to the group (find by bot link) </p> <p> </p> <p>Select a group name of your choice (where you want to receive your alerts) </p> <p> </p> <p>Verify that the newly created group exists </p> <p> </p>"},{"location":"monitoring/#configure-contact-point","title":"Configure contact point","text":"<p>Proceed to alert setup </p> <p> </p> <p>Create a new contact point </p> <p> </p> <p>Select a name and type for telegram alert spec </p> <p> </p> <p>Enter you bot API token, see previous section on configuring your telegram bot (data should still be in your telegram)</p> <p>Enter your chat ID for your group (can be gathered by viewing the group info) will only work if you bot has been started. See previous section <pre><code>https://api.telegram.org/bot&lt;YOUR_API_TOKEN_KEY&gt;/getUpdates \n</code></pre> Your chat ID with the numeric digits prefixed by the '-'</p> <p>Alternatively, you can open telegram web, proceed to the group and retrieve your chat ID which is indicated by your url after the # sign</p> <p>Leave your message empty and notification settings empty and click on save</p> <p> </p>"},{"location":"monitoring/#cconfigure-alerting-in-grafana-to-post-alert-messages-to-telegram","title":"CConfigure Alerting in Grafana to post alert messages to telegram","text":"<p>Browse to your dashboard and select the graph you want to be alerted on (in my case number of connections)</p> <p> </p> <p>Click on alert for the graph you are editing</p> <p> </p> <p>Create an alert</p> <p> </p> <p>Choose alert condition (in my test, I alerted on connections exceeding 3)</p> <p> </p> <p>Put annotations to your alert notifications to make it more fruitfull</p> <p> </p> <p>Save your alert and exit. Then if you condition is triggered, await your alert on telegram</p> <p> </p> <p>When the issue has cleared (I have deleted the alert to just show the resolved status, ignore the value)</p> <p> </p>"},{"location":"nodeos/","title":"Nodeos","text":"<p>nodeos is the core service daemon that runs on every EOSIO node. It can be configured to process smart contracts, validate transactions, produce blocks containing valid transactions, and confirm blocks to record them on the blockchain.</p> <p>Depending on the type of system (whether it is baremetal or cloud) you have configured to run your state history, you might want to isolate the cores to prevent the operating system from utilizing them for certain system processes.</p> <p>Linux provides the capability to allocate certain process ID's (identifier used to uniquely identify a process) to specific CPU cores. The benefit here is the ability to seperate CPU bound applications and have a specific affinity set to get most out of the core frequencies. </p>"},{"location":"nodeos/#scenarios","title":"Scenarios","text":"<ul> <li>You have to run one of Atomic, History or Hyperion API with a SHIP instance (which is risky, but in terms of budgeting preferences), but would like the chain data to be syncronized in a performent manner, while the API requires as much cores as it is entitled too.</li> </ul> <p>To enable process/core isolation, you need to add the following configuration to your Kernel Loader</p> <p>NOTE: In the example, the intention was to isolate cores 7,9,11. Total amount of CPU's = 12</p> <p>NOTE: cpu id's start at 0</p> <p><pre><code>GRUB_CMDLINE_LINUX_DEFAULT=\"isolcpus=8,10,12\"\n</code></pre> Update your grub configuration and reboot your system once <pre><code>update-grub &amp;&amp; reboot\n</code></pre> If you would like to test your recent changes, you can install a stress utility and launch a stress test against all of the cores to ensure the isolated cores are not used since they should be unavailable unless processes has been delegated to those <pre><code>apt install -y stress\n</code></pre> To perform the test against all cpu's <pre><code>stress -c 12\n</code></pre> To schedule a process on a particular core <pre><code>taskset -cp &lt;isolated core id&gt; &lt;process id/id's&gt;\n</code></pre> Another way to optimize the nodeos processes would be to set the scheduling parameters to type (SCHED_BATCH) which is designed for non-interactive, CPU-bound applications. It uses longer timeslices (short time frame that gets assigned to process for CPU execution). These processes are normally scheduled with lower priority, but may result in considerable speed improvements.</p> <p>To schedule a process with SCHED_BATCH parameter <pre><code>schedtool -B &lt;process id/id's&gt;\n</code></pre></p>"},{"location":"nodeos/#nodeos-startup-script","title":"nodeos startup script","text":"<p>By this time you should be aware that nodes should execute as a daemon process to prevent the need of an interactive session during the lifetime of the process. If your session ends without gracefull termination, your state history will be out-of-sync, and you will need to replay several sets of data which is time consuming, as well put the dependent systems at risk as they should be level with the headblock</p> <p>When launching nodeos, there will be 3 process id's which will be registered. Ideally, you would like all 3 process id's allocated to seperate cores which is isolated from system processes (benefits explained above) You can add the following to your startup script to allocate the nodeos pids automatically.</p> <p>NOTE: (This is still based of the example of having isolated cores 7,9,11). This will schedule the nodes pids each on a seperate isolated CPU and schedule it as a SCHED_BATCH service <pre><code>PIDS=`pidof nodeos`\narr=($PIDS)\nNODEPROC1=${arr[0]}\nNODEPROC2=${arr[1]}\nNODEPROC3=${arr[2]}\necho $NODEPROC1\necho $NODEPROC2\necho $NODEPROC3\ntaskset -cp 7 $NODEPROC1 &amp;&amp; schedtool -B $NODEPROC1\ntaskset -cp 9 $NODEPROC2 &amp;&amp; schedtool -B $NODEPROC2\ntaskset -cp 11 $NODEPROC3 &amp;&amp; schedtool -B $NODEPROC3\n</code></pre></p>"},{"location":"postgresql/","title":"PostgreSQL","text":"<p>Take note of ubuntu performance performance when configuring a Atomic instance</p>"},{"location":"postgresql/#the-following-will-be-covered-here","title":"The following will be covered here","text":"<ul> <li>Log into postgresql</li> <li>Recover database backup</li> <li>Perform database backup</li> </ul>"},{"location":"postgresql/#perform-a-database-dump-backup-of-your-atomic-database","title":"Perform a database dump (backup) of your atomic database","text":"<p>Connect using the postgres user</p> <pre><code>su - postgres\n</code></pre> <p>List your databases to retrieve the exact name of your wax atomic database</p> <pre><code>psql\n\\l\n</code></pre> <p>Results should look something like this</p> <pre><code>postgres-# \\l+\n                                                                       List of databases\n           Name           |  Owner   | Encoding | Collate |  Ctype  |   Access privileges   |  Size   | Tablespace |                Description                 \n--------------------------+----------+----------+---------+---------+-----------------------+---------+------------+--------------------------------------------\n api-wax-mainnet-atomic-1 | postgres | UTF8     | C.UTF-8 | C.UTF-8 |                       | 21 GB   | pg_default | \n postgres                 | postgres | UTF8     | C.UTF-8 | C.UTF-8 |                       | 8601 kB | pg_default | default administrative connection database\n template0                | postgres | UTF8     | C.UTF-8 | C.UTF-8 | =c/postgres          +| 8377 kB | pg_default | unmodifiable empty database\n                          |          |          |         |         | postgres=CTc/postgres |         |            | \n template1                | postgres | UTF8     | C.UTF-8 | C.UTF-8 | =c/postgres          +| 8545 kB | pg_default | default template for new databases\n                          |          |          |         |         | postgres=CTc/postgres |         |            | \n(4 rows)\n</code></pre> <p>Perform a database dump of your atomic database</p> <pre><code>pg_dump -Fc \"api-wax-mainnet-atomic-1\" &gt; /data/postgres_backups/atomictest.dump\n</code></pre>"},{"location":"snapshots/","title":"Hyperion Snapshots","text":"<p>6 week snapshot listing </p> <p>It is clear that the elastic indexes are growing at a rapid pace in terms of sizing.  This will make it extremely difficult to maintain full hyperion history, or even get started since you need to pull an entire snapshot before any maintenance can be done.  Since the OIG still validate 6 weeks of Hyperion history as partial and for running partial Hyperion you receive scoring, I started creating snapshots daily that contains all data later than 6 weeks. </p> <p>Please note the following on the snapshots</p> <ul> <li>Validated for no missing blocks (used hyperion 3.3.6) </li> <li>Syned up to headblock at the time of the snapshot </li> <li>The snapshot naming convention is as follows </li> </ul> <p> </p> <p>The name (for instance 11-09-22) will be the identifier for which data is contained in the snapshot.  For instance, in the case above, the indexes will contain data populated from 11-09-2022 00:00:00 GMT +2</p> <p>To restore the snapshot to your hyperion instance, the following will be necessary</p> <p>Update your Elasticsearch.yml to trust the repository from which you will download the snapshotted data</p> <pre><code>repositories.url.allowed_urls: [\"https://snapshots.oiac.io/downloads/partialsnapshots/\"]\nservice elasticsearch restart\n</code></pre> <p>Add the repository using dev tools in elasticsearch  Note: I named the repository partial-mainnet-snapshots</p> <pre><code>PUT _snapshot/partial-mainnet-snapshots\n{\n   \"type\": \"url\",\n   \"settings\": {\n       \"url\": \"https://snapshots.oiac.io/downloads/partialsnapshots/\",\n       \"max_restore_bytes_per_sec\": \"4gb\",\n       \"max_snapshot_bytes_per_sec\": \"4gb\" \n   }\n}\n</code></pre> <p>List the snapshots</p> <pre><code>GET _snapshot/partial-mainnet-snapshots/_all\n</code></pre> <p>Restore the snapshot you want (in your case the latest will make sense) </p> <p> </p> <pre><code>POST _snapshot/partial-mainnet-snapshots/02-10-2022-partial-hyperion-yrvehaihro2tg-wyx9gvja/_restore\n{\n  \"indices\": \"*\"\n}\n</code></pre>"},{"location":"snapshots/#current-snapshots","title":"Current Snapshots","text":"<p>Please find the latest hyperion snapshots for partial restores =&gt; here</p>"},{"location":"snapshots/#latest-restore-validated","title":"Latest Restore Validated","text":"<pre><code>POST _snapshot/snaps-30/10-10-2022-partial-hyperion-fjo3ryzttdubtrzb-wxocg/_restore\n{\n  \"indices\": \"*\"\n}\n</code></pre> Index Status Last activity Shards completed Shards in progress wax-perm-v1 Complete Nov 23, 2022\u00a07:36 AM GMT+2 2 0 wax-abi-v1 Complete Nov 23, 2022\u00a07:35 AM GMT+2 2 0 wax-delta-v1-000021 Complete Nov 23, 2022\u00a07:35 AM GMT+2 4 0 wax-block-v1 Complete Nov 23, 2022\u00a07:34 AM GMT+2 2 0 wax-table-accounts-v1 Complete Nov 23, 2022\u00a07:34 AM GMT+2 2 0 wax-table-voters-v1 Complete Nov 23, 2022\u00a07:31 AM GMT+2 2 0 wax-link-v1 Complete Nov 23, 2022\u00a07:31 AM GMT+2 2 0 wax-table-proposals-v1 Complete Nov 23, 2022\u00a07:31 AM GMT+2 2 0 wax-logs Complete Nov 23, 2022\u00a07:17 AM GMT+2 1 0 wax-action-v1-000021 Complete Nov 23, 2022\u00a06:42 AM GMT+2 4 0 wax-delta-v1-000022 Complete Nov 23, 2022\u00a04:30 AM GMT+2 4 0 wax-action-v1-000022 Complete Nov 23, 2022\u00a02:26 AM GMT+2 4 0 wax-logs-v1 Complete Nov 22, 2022\u00a09:25 PM GMT+2 2 0 <pre><code>curl -v http://127.0.0.1:7000/v2/health | json_pp\n*   Trying 127.0.0.1:7000...\n* TCP_NODELAY set\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0* Connected to 127.0.0.1 (127.0.0.1) port 7000 (#0)\n&gt; GET /v2/health HTTP/1.1\n&gt; Host: 127.0.0.1:7000\n&gt; User-Agent: curl/7.68.0\n&gt; Accept: */*\n&gt; \n* Mark bundle as not supporting multiuse\n&lt; HTTP/1.1 200 OK\n&lt; vary: Origin\n&lt; access-control-allow-origin: *\n&lt; content-type: application/json; charset=utf-8\n&lt; content-length: 1016\n&lt; Date: Thu, 24 Nov 2022 02:36:23 GMT\n&lt; Connection: keep-alive\n&lt; Keep-Alive: timeout=5\n&lt; \n{ [1016 bytes data]\n100  1016  100  1016    0     0   1587      0 --:--:-- --:--:-- --:--:--  1587\n* Connection #0 to host 127.0.0.1 left intact\n{\n   \"features\" : {\n      \"deferred_trx\" : false,\n      \"failed_trx\" : false,\n      \"index_all_deltas\" : true,\n      \"index_deltas\" : true,\n      \"index_transfer_memo\" : true,\n      \"resource_limits\" : false,\n      \"resource_usage\" : false,\n      \"streaming\" : {\n         \"deltas\" : true,\n         \"enable\" : true,\n         \"traces\" : true\n      },\n      \"tables\" : {\n         \"accounts\" : true,\n         \"proposals\" : true,\n         \"voters\" : true\n      }\n   },\n   \"health\" : [\n      {\n         \"service\" : \"RabbitMq\",\n         \"status\" : \"OK\",\n         \"time\" : 1669257382974\n      },\n      {\n         \"service\" : \"NodeosRPC\",\n         \"service_data\" : {\n            \"chain_id\" : \"1064487b3cd1a897ce03ae5b6a865651747e2e152090f99c1d19d44e01aea5a4\",\n            \"head_block_num\" : 215586809,\n            \"head_block_time\" : \"2022-11-24T02:36:23.000\",\n            \"last_irreversible_block\" : 215586474,\n            \"time_offset\" : -49\n         },\n         \"status\" : \"OK\",\n         \"time\" : 1669257382951\n      },\n      {\n         \"service\" : \"Elasticsearch\",\n         \"service_data\" : {\n            \"active_shards\" : \"100.0%\",\n            \"first_indexed_block\" : 207796238,\n            \"head_offset\" : -1,\n            \"last_indexed_block\" : 215586810,\n            \"missing_blocks\" : 0,\n            \"missing_pct\" : \"0.00%\",\n            \"total_indexed_blocks\" : 7790572\n         },\n         \"status\" : \"OK\",\n         \"time\" : 1669257383566\n      }\n   ],\n   \"host\" : \"hyperion.oiac.io\",\n   \"query_time_ms\" : 632.49,\n   \"version\" : \"3.3.6\",\n   \"version_hash\" : \"3b74decbe6b07cad76cedafa57ef2746f0588bde\"\n}\n</code></pre> <p>Indexer </p> <pre><code>/root/.pm2/logs/wax-indexer-out.log last 15 lines:\n0|wax-inde | 2022-11-24T02:41:11: [3003206 - 00_master] W:40 | R:2 | C:2 | A:694.6 | D:989.6 | I:1828.4\n0|wax-inde | 2022-11-24T02:41:16: [3003206 - 00_master] W:40 | R:2 | C:2 | A:581 | D:580.8 | I:1465.8\n0|wax-inde | 2022-11-24T02:41:21: [3003206 - 00_master] W:40 | R:2 | C:2 | A:649.4 | D:905.4 | I:1807.8\n0|wax-inde | 2022-11-24T02:41:26: [3003206 - 00_master] W:40 | R:2 | C:1.8 | A:742.4 | D:980.6 | I:1881.8\n0|wax-inde | 2022-11-24T02:41:31: [3003206 - 00_master] W:40 | R:1.8 | C:2 | A:827 | D:1069 | I:2099\n0|wax-inde | 2022-11-24T02:41:36: [3003206 - 00_master] W:40 | R:2.2 | C:2.2 | A:740.2 | D:943.6 | I:1933.8\n0|wax-inde | 2022-11-24T02:41:41: [3003206 - 00_master] W:40 | R:2 | C:2 | A:552.2 | D:787.2 | I:1440\n0|wax-inde | 2022-11-24T02:41:46: [3003206 - 00_master] W:40 | R:2 | C:2 | A:655.8 | D:778.2 | I:1792.6\n0|wax-inde | 2022-11-24T02:41:51: [3003206 - 00_master] W:40 | R:2 | C:1.8 | A:877.6 | D:1170 | I:2207.6\n0|wax-inde | 2022-11-24T02:41:56: [3003206 - 00_master] W:40 | R:2 | C:2.2 | A:691 | D:1012.6 | I:1707.2\n0|wax-inde | 2022-11-24T02:42:01: [3003206 - 00_master] W:40 | R:2 | C:2 | A:506.2 | D:611.4 | I:1501.2\n0|wax-inde | 2022-11-24T02:42:06: [3003206 - 00_master] W:40 | R:2 | C:2 | A:706 | D:970.4 | I:1890.2\n0|wax-inde | 2022-11-24T02:42:11: [3003206 - 00_master] W:40 | R:2 | C:2 | A:1009.2 | D:1437.8 | I:2538.8\n0|wax-inde | 2022-11-24T02:42:16: [3003206 - 00_master] W:40 | R:2 | C:2 | A:860.2 | D:954 | I:2359.4\n0|wax-inde | 2022-11-24T02:42:21: [3003206 - 00_master] W:40 | R:2 | C:2 | A:878.6 | D:1104.6 | I:2242.4\n</code></pre> <p>Estimated restore time (based on only having 4 shards available) </p> <pre><code>21:25 -&gt; 07:36 (9 hours, 11 minutes)\nNote that you can restore quicker if you have more nodes available (i.e. more shards to do the work)\n</code></pre>"},{"location":"ubuntu/","title":"Ubuntu","text":""},{"location":"ubuntu/#set-the-performance-governer-baremetal-instances","title":"Set the performance governer (baremetal instances)","text":"<p>The CPUfreq governor \"userspace\" allows the user, or any userspace program running with UID \"root\", to set the CPU to a specific frequency by making a sysfs file \"scaling_setspeed\" available in the CPU-device directory</p> <p>By default, service providers will not have their performance governer set to \"performance mode\" to save on power and costs + extend the lifetime of peripherals. This is a waste and you should be utilizing every core at maximum capacity as you see fit.</p> <p>Governor summary - on demand: Expand or reduce resource consumption based on demand. - Coservative: It is a profile by which you try to keep the level of spending at the basic levels. - Performance: It is the most devouring of resources since it makes the system available to the tasks trying to give the maximum possible performance in everything. - It is the most resource-saving profile, reducing energy and system resource consumption to a minimum.</p> <p>To have a collective view of the governer set on each core, you can issue the following command <pre><code>cat /sys/devices/system/cpu/cpu*/cpufreq/scaling_gov*\n</code></pre> To set each core profile to performance, you can issue the following <pre><code>for x in /sys/devices/system/cpu/cpu*/cpufreq/;do echo performance &gt; $x/scaling_governor; done\n</code></pre> If you would like certain cores on powersave and some on performance <pre><code>for x in /sys/devices/system/cpu/cpu[0-6]/cpufreq/;do echo performance &gt; $x/scaling_governor; done\nfor x in /sys/devices/system/cpu/cpu[7-11]/cpufreq/;do echo powersave &gt; $x/scaling_governor; done\n</code></pre></p>"},{"location":"ubuntu/#scaling-maximum-frequency","title":"Scaling maximum frequency","text":"<p>The majority of modern processors are capable of operating in a number of different clock frequency and voltage configurations, often referred to as Operating Performance Points or P-states (in ACPI terminology). As a rule, the higher the clock frequency and the higher the voltage, the more instructions can be retired by the CPU over a unit of time, but also the higher the clock frequency and the higher the voltage, the more energy is consumed over a unit of time (or the more power is drawn) by the CPU in the given P-state.</p> <p>When attached to a policy object, this governor causes the highest frequency, within the scaling_max_freq policy limit, to be requested for that policy. The request is made once at that time the governor for the policy is set to performance and whenever the scaling_max_freq or scaling_min_freq policy limits change after that.</p> <p>To find your maximum frequency for each CPU (remember that this could differ between cores), issue the following <pre><code>cpufreq-info\n</code></pre> In particular, we focus on CPU 31 which has the following scaling policy set <pre><code>analyzing CPU 31:\n  driver: acpi-cpufreq\n  CPUs which run at the same hardware frequency: 31\n  CPUs which need to have their frequency coordinated by software: 31\n  maximum transition latency: 4294.55 ms.\n  hardware limits: 2.20 GHz - 3.40 GHz\n  available frequency steps: 3.40 GHz, 2.80 GHz, 2.20 GHz\n  available cpufreq governors: conservative, ondemand, userspace, powersave, performance, schedutil\n  current policy: frequency should be within 2.20 GHz and 3.40 GHz.\n                  The governor \"performance\" may decide which speed to use\n                  within this range.\n  current CPU frequency is 3.40 GHz (asserted by call to hardware).\n  cpufreq stats: 3.40 GHz:5.94%, 2.80 GHz:0.37%, 2.20 GHz:93.68%  (685562)\n</code></pre> You should now have a list of CPU's with the maximum limit and you will be able to adjust the policy scale frequencies to maximum</p> <p>NOTE: In this example, I am scaling CPU 31 to allow policy frequency between maximum states <pre><code>cpufreq-set -c 31 -r -g performance --min 3400000 --max 3400000\nResult =&gt; \nanalyzing CPU 31:\n  driver: acpi-cpufreq\n  CPUs which run at the same hardware frequency: 31\n  CPUs which need to have their frequency coordinated by software: 31\n  maximum transition latency: 4294.55 ms.\n  hardware limits: 2.20 GHz - 3.40 GHz\n  available frequency steps: 3.40 GHz, 2.80 GHz, 2.20 GHz\n  available cpufreq governors: conservative, ondemand, userspace, powersave, performance, schedutil\n  current policy: frequency should be within 3.40 GHz and 3.40 GHz.\n                  The governor \"performance\" may decide which speed to use\n                  within this range.\n  current CPU frequency is 3.40 GHz (asserted by call to hardware).\n  cpufreq stats: 3.40 GHz:5.95%, 2.80 GHz:0.37%, 2.20 GHz:93.68%  (685562)\n</code></pre> if you would like a bulk update on all CPU's to have the minimum scale = maximum scale <pre><code>for x in /sys/devices/system/cpu/cpu*/cpufreq/;do echo 3400000 &gt; $x/scaling_min_freq; done\nResult =&gt;\ncpufreq-info | grep policy\n  current policy: frequency should be within 3.40 GHz and 3.40 GHz.\n  current policy: frequency should be within 3.40 GHz and 3.40 GHz.\n  current policy: frequency should be within 3.40 GHz and 3.40 GHz.\n  current policy: frequency should be within 3.40 GHz and 3.40 GHz.\n  current policy: frequency should be within 3.40 GHz and 3.40 GHz.\n  current policy: frequency should be within 3.40 GHz and 3.40 GHz.\n  current policy: frequency should be within 3.40 GHz and 3.40 GHz.\n  current policy: frequency should be within 3.40 GHz and 3.40 GHz.\n  current policy: frequency should be within 3.40 GHz and 3.40 GHz.\n  current policy: frequency should be within 3.40 GHz and 3.40 GHz.\n  current policy: frequency should be within 3.40 GHz and 3.40 GHz.\n  current policy: frequency should be within 3.40 GHz and 3.40 GHz.\n  current policy: frequency should be within 3.40 GHz and 3.40 GHz.\n  current policy: frequency should be within 3.40 GHz and 3.40 GHz.\n  current policy: frequency should be within 3.40 GHz and 3.40 GHz.\n  current policy: frequency should be within 3.40 GHz and 3.40 GHz.\n  current policy: frequency should be within 3.40 GHz and 3.40 GHz.\n  current policy: frequency should be within 3.40 GHz and 3.40 GHz.\n  current policy: frequency should be within 3.40 GHz and 3.40 GHz.\n  current policy: frequency should be within 3.40 GHz and 3.40 GHz.\n  current policy: frequency should be within 3.40 GHz and 3.40 GHz.\n  current policy: frequency should be within 3.40 GHz and 3.40 GHz.\n  current policy: frequency should be within 3.40 GHz and 3.40 GHz.\n  current policy: frequency should be within 3.40 GHz and 3.40 GHz.\n  current policy: frequency should be within 3.40 GHz and 3.40 GHz.\n  current policy: frequency should be within 3.40 GHz and 3.40 GHz.\n  current policy: frequency should be within 3.40 GHz and 3.40 GHz.\n  current policy: frequency should be within 3.40 GHz and 3.40 GHz.\n  current policy: frequency should be within 3.40 GHz and 3.40 GHz.\n  current policy: frequency should be within 3.40 GHz and 3.40 GHz.\n  current policy: frequency should be within 3.40 GHz and 3.40 GHz.\n  current policy: frequency should be within 3.40 GHz and 3.40 GHz.\n</code></pre></p>"},{"location":"wireguard/","title":"VPN","text":"<p>WireGuard is a secure network tunnel, operating at layer 3, implemented as a kernel virtual network interface for Linux, which aims to replace both IPsec for most use cases, as well as popular user space and/or TLS-based solutions like OpenVPN, while being more secure, more performant, and easier to use. It is essential to have traffic flowing between your hosts flowing via an encrypted tunnel.</p>"},{"location":"wireguard/#install-wireguard","title":"Install wireguard","text":"<p>NOTE: In this example, I will configure HOST-A on 172.168.25.1 and HOST-B on 172.168.25.2</p> <p>On HOST-A install the wireguard suite as follows <pre><code>sudo apt install wireguard\nwg genkey &gt; private\nwg pubkey &lt; private\nip link add wg0 type wireguard\nip addr add 172.168.25.1/24 dev wg0\nwg set wg0 private-key ./private\nip link set wg0 up\nwg showconf wg0 &gt; /etc/wireguard/wg0.conf\necho \"SaveConfig = true\" &gt;&gt; /etc/wireguard/wg0.conf\nwg-quick save wg0\nwg show\n</code></pre></p> <p>Take note of interface: wg0 public key and listening port on HOST-A  </p> <p>Open up the firewall for the port on which wg expose the endpoint <pre><code>firewall-cmd --add-port=&lt;listening port&gt;/tcp --permanent\nservice firewalld reload\n</code></pre></p> <p>On HOST-B configure as follows <pre><code>sudo apt install wireguard\nwg genkey &gt; private\nwg pubkey &lt; private\nip link add wg0 type wireguard\nip addr add 172.168.25.2/24 dev wg0\nwg set wg0 private-key ./private\nip link set wg0 up\nwg showconf wg0 &gt; /etc/wireguard/wg0.conf\necho \"SaveConfig = true\" &gt;&gt; /etc/wireguard/wg0.conf\nwg-quick save wg0\n</code></pre></p> <p>Take note of interface: wg0 public key and listening port on HOST-B  </p> <pre><code>firewall-cmd --add-port=&lt;listening port&gt;/tcp --permanent\nservice firewalld reload\n</code></pre> <p>NOTE: Having both HOST-A and HOST-B public key as well as port, you can now establish a vpn tunnel between the 2.</p> <p>On HOST-A execute the following =&gt; <pre><code>wg set wg0 peer &lt;HOST-B public key&gt; allowed-ips &lt;172.168.25.2&gt;/32 endpoint &lt;public ip of HOST-B and port (ip:port)&gt;\nwg set wg0 peer &lt;HOST-B public key&gt; persistent-keepalive 25\nwg-quick down wg0\nwg-quick up wg0\n</code></pre></p> <p>On HOST-B execute the following =&gt; <pre><code>wg set wg0 peer &lt;HOST-A public key&gt; allowed-ips &lt;172.168.25.1&gt;/32 endpoint &lt;public ip of HOST-A and port (ip:port)&gt;\nwg set wg0 peer &lt;HOST-A public key&gt; persistent-keepalive 25\nwg-quick down wg0\nwg-quick up wg0\n</code></pre> The persistent-keepalive between HOST-A and HOST-B will send a packet to each peer to keep tunnel active  Both hosts should now be reachable via a secure VPN tunnel </p>"}]}